{
    "matches": [
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "dataset": "Dev"
                },
                "measures": "[BLEU]",
                "outcomes": "[9.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.5"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.3, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.3, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.3, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.3, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.3, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.3, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.3, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.3, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.3, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.3, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.3, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "dataset": "Dev"
                },
                "measures": "[BLEU]",
                "outcomes": "[15.3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "dataset": "Dev"
                },
                "measures": "[BLEU]",
                "outcomes": "[15.3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "dataset": "Dev"
                },
                "measures": "[BLEU]",
                "outcomes": "[15.3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "dataset": "Dev"
                },
                "measures": "[BLEU]",
                "outcomes": "[15.3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[14.1, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[14.1, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[14.1, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[14.1, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[14.1, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[14.1, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[14.1, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[14.1, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[14.1, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[14.1, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Dev"
                },
                "measures": "[BLEU]",
                "outcomes": "[16.4]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Dev"
                },
                "measures": "[BLEU]",
                "outcomes": "[16.4]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Dev"
                },
                "measures": "[BLEU]",
                "outcomes": "[16.4]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Dev"
                },
                "measures": "[BLEU]",
                "outcomes": "[16.4]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Dev"
                },
                "measures": "[BLEU]",
                "outcomes": "[16.4]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Dev"
                },
                "measures": "[BLEU]",
                "outcomes": "[16.4]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Dev"
                },
                "measures": "[BLEU]",
                "outcomes": "[16.4]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.1, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.1, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.1, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.1, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.1, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.1, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.1, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.1, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "dataset": "Test"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.1, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "evaluation_method": "human evaluation",
                    "sample_size": "100",
                    "sample_source": "test set",
                    "evaluators": "three students proficient in English",
                    "lcr_scoring": "0 or 1",
                    "grammar_scoring": "1 to 5",
                    "lcr_decision_method": "majority vote",
                    "grammar_score_calculation": "average"
                }
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "evaluation_method": "human evaluation",
                    "sample_size": "100",
                    "sample_source": "test set",
                    "evaluators": "three students proficient in English",
                    "lcr_scoring": "0 or 1",
                    "grammar_scoring": "1 to 5",
                    "lcr_decision_method": "majority vote",
                    "grammar_score_calculation": "average"
                }
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "evaluation_method": "human evaluation",
                    "sample_size": "100",
                    "sample_source": "test set",
                    "evaluators": "three students proficient in English",
                    "lcr_scoring": "0 or 1",
                    "grammar_scoring": "1 to 5",
                    "lcr_decision_method": "majority vote",
                    "grammar_score_calculation": "average"
                }
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "evaluation_method": "human evaluation",
                    "sample_size": "100",
                    "sample_source": "test set",
                    "evaluators": "three students proficient in English",
                    "lcr_scoring": "0 or 1",
                    "grammar_scoring": "1 to 5",
                    "lcr_decision_method": "majority vote",
                    "grammar_score_calculation": "average"
                }
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "evaluation_method": "human evaluation",
                    "sample_size": "100",
                    "sample_source": "test set",
                    "evaluators": "three students proficient in English",
                    "lcr_scoring": "0 or 1",
                    "grammar_scoring": "1 to 5",
                    "lcr_decision_method": "majority vote",
                    "grammar_score_calculation": "average"
                }
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "evaluation_method": "human evaluation",
                    "sample_size": "100",
                    "sample_source": "test set",
                    "evaluators": "three students proficient in English",
                    "lcr_scoring": "0 or 1",
                    "grammar_scoring": "1 to 5",
                    "lcr_decision_method": "majority vote",
                    "grammar_score_calculation": "average"
                }
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "evaluation_method": "human evaluation",
                    "sample_size": "100",
                    "sample_source": "test set",
                    "evaluators": "three students proficient in English",
                    "lcr_scoring": "0 or 1",
                    "grammar_scoring": "1 to 5",
                    "lcr_decision_method": "majority vote",
                    "grammar_score_calculation": "average"
                }
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "evaluation_method": "human evaluation",
                    "sample_size": "100",
                    "sample_source": "test set",
                    "evaluators": "three students proficient in English",
                    "lcr_scoring": "0 or 1",
                    "grammar_scoring": "1 to 5",
                    "lcr_decision_method": "majority vote",
                    "grammar_score_calculation": "average"
                }
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "evaluation_method": "human evaluation",
                    "sample_size": "100",
                    "sample_source": "test set",
                    "evaluators": "three students proficient in English",
                    "lcr_scoring": "0 or 1",
                    "grammar_scoring": "1 to 5",
                    "lcr_decision_method": "majority vote",
                    "grammar_score_calculation": "average"
                }
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        }
    ],
    "total_extracted_claims": 7,
    "total_ground_truth_claims": 12,
    "number_of_matches": 3
}