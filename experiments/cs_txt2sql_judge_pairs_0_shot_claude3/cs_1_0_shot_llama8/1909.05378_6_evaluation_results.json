{
    "matches": [
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Template",
                    "BLEU": "9.5",
                    "LCR": "41.0",
                    "Grammar": "4.0"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.5, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Template",
                    "BLEU": "9.5",
                    "LCR": "41.0",
                    "Grammar": "4.0"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.5, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Template",
                    "BLEU": "9.5",
                    "LCR": "41.0",
                    "Grammar": "4.0"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.5, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Template",
                    "BLEU": "9.5",
                    "LCR": "41.0",
                    "Grammar": "4.0"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.5, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Template",
                    "BLEU": "9.5",
                    "LCR": "41.0",
                    "Grammar": "4.0"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.5, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Template",
                    "BLEU": "9.5",
                    "LCR": "41.0",
                    "Grammar": "4.0"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.5, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Template",
                    "BLEU": "9.5",
                    "LCR": "41.0",
                    "Grammar": "4.0"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.5, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Template",
                    "BLEU": "9.5",
                    "LCR": "41.0",
                    "Grammar": "4.0"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.5, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Template",
                    "BLEU": "9.5",
                    "LCR": "41.0",
                    "Grammar": "4.0"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.5, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Template",
                    "BLEU": "9.5",
                    "LCR": "41.0",
                    "Grammar": "4.0"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.5, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Template",
                    "BLEU": "9.5",
                    "LCR": "41.0",
                    "Grammar": "4.0"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.5, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Template",
                    "BLEU": "9.5",
                    "LCR": "41.0",
                    "Grammar": "4.0"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[9.5, 41.0, 4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "BLEU": "15.3",
                    "LCR": "27.0",
                    "Grammar": "3.5"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.3, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "BLEU": "15.3",
                    "LCR": "27.0",
                    "Grammar": "3.5"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.3, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "BLEU": "15.3",
                    "LCR": "27.0",
                    "Grammar": "3.5"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.3, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "BLEU": "15.3",
                    "LCR": "27.0",
                    "Grammar": "3.5"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.3, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "BLEU": "15.3",
                    "LCR": "27.0",
                    "Grammar": "3.5"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.3, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "BLEU": "15.3",
                    "LCR": "27.0",
                    "Grammar": "3.5"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.3, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "BLEU": "15.3",
                    "LCR": "27.0",
                    "Grammar": "3.5"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.3, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "BLEU": "15.3",
                    "LCR": "27.0",
                    "Grammar": "3.5"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.3, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "BLEU": "15.3",
                    "LCR": "27.0",
                    "Grammar": "3.5"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.3, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "BLEU": "15.3",
                    "LCR": "27.0",
                    "Grammar": "3.5"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.3, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "BLEU": "15.3",
                    "LCR": "27.0",
                    "Grammar": "3.5"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.3, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "BLEU": "15.3",
                    "LCR": "27.0",
                    "Grammar": "3.5"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[15.3, 27.0, 3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "BLEU": "16.4",
                    "LCR": "35.0",
                    "Grammar": "3.6"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[16.4, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "BLEU": "16.4",
                    "LCR": "35.0",
                    "Grammar": "3.6"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[16.4, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "BLEU": "16.4",
                    "LCR": "35.0",
                    "Grammar": "3.6"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[16.4, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "BLEU": "16.4",
                    "LCR": "35.0",
                    "Grammar": "3.6"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[16.4, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "BLEU": "16.4",
                    "LCR": "35.0",
                    "Grammar": "3.6"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[16.4, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "BLEU": "16.4",
                    "LCR": "35.0",
                    "Grammar": "3.6"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[16.4, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "BLEU": "16.4",
                    "LCR": "35.0",
                    "Grammar": "3.6"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[16.4, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "BLEU": "16.4",
                    "LCR": "35.0",
                    "Grammar": "3.6"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[16.4, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "BLEU": "16.4",
                    "LCR": "35.0",
                    "Grammar": "3.6"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[16.4, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "BLEU": "16.4",
                    "LCR": "35.0",
                    "Grammar": "3.6"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[16.4, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "BLEU": "16.4",
                    "LCR": "35.0",
                    "Grammar": "3.6"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[16.4, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "BLEU": "16.4",
                    "LCR": "35.0",
                    "Grammar": "3.6"
                },
                "measures": "[BLEU, LCR, Grammar]",
                "outcomes": "[16.4, 35.0, 3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Value": "Papineni et al. (2002)"
                },
                "measures": "[BLEU]",
                "outcomes": "[Papineni et al. (2002)]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Value": "Papineni et al. (2002)"
                },
                "measures": "[BLEU]",
                "outcomes": "[Papineni et al. (2002)]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Value": "Papineni et al. (2002)"
                },
                "measures": "[BLEU]",
                "outcomes": "[Papineni et al. (2002)]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Value": "Papineni et al. (2002)"
                },
                "measures": "[BLEU]",
                "outcomes": "[Papineni et al. (2002)]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Value": "Papineni et al. (2002)"
                },
                "measures": "[BLEU]",
                "outcomes": "[Papineni et al. (2002)]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Value": "Papineni et al. (2002)"
                },
                "measures": "[BLEU]",
                "outcomes": "[Papineni et al. (2002)]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Value": "Papineni et al. (2002)"
                },
                "measures": "[BLEU]",
                "outcomes": "[Papineni et al. (2002)]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Value": "Papineni et al. (2002)"
                },
                "measures": "[BLEU]",
                "outcomes": "[Papineni et al. (2002)]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Value": "Papineni et al. (2002)"
                },
                "measures": "[BLEU]",
                "outcomes": "[Papineni et al. (2002)]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Value": "Papineni et al. (2002)"
                },
                "measures": "[BLEU]",
                "outcomes": "[Papineni et al. (2002)]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Value": "Papineni et al. (2002)"
                },
                "measures": "[BLEU]",
                "outcomes": "[Papineni et al. (2002)]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Value": "Papineni et al. (2002)"
                },
                "measures": "[BLEU]",
                "outcomes": "[Papineni et al. (2002)]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Value": "0 or 1"
                },
                "measures": "[LCR]",
                "outcomes": "[0 or 1]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Value": "0 or 1"
                },
                "measures": "[LCR]",
                "outcomes": "[0 or 1]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Value": "0 or 1"
                },
                "measures": "[LCR]",
                "outcomes": "[0 or 1]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Value": "0 or 1"
                },
                "measures": "[LCR]",
                "outcomes": "[0 or 1]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Value": "0 or 1"
                },
                "measures": "[LCR]",
                "outcomes": "[0 or 1]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Value": "0 or 1"
                },
                "measures": "[LCR]",
                "outcomes": "[0 or 1]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Value": "0 or 1"
                },
                "measures": "[LCR]",
                "outcomes": "[0 or 1]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Value": "0 or 1"
                },
                "measures": "[LCR]",
                "outcomes": "[0 or 1]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Value": "0 or 1"
                },
                "measures": "[LCR]",
                "outcomes": "[0 or 1]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Value": "0 or 1"
                },
                "measures": "[LCR]",
                "outcomes": "[0 or 1]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Value": "0 or 1"
                },
                "measures": "[LCR]",
                "outcomes": "[0 or 1]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Value": "0 or 1"
                },
                "measures": "[LCR]",
                "outcomes": "[0 or 1]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Value": "1 to 5"
                },
                "measures": "[Grammar]",
                "outcomes": "[1 to 5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Value": "1 to 5"
                },
                "measures": "[Grammar]",
                "outcomes": "[1 to 5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Value": "1 to 5"
                },
                "measures": "[Grammar]",
                "outcomes": "[1 to 5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Value": "1 to 5"
                },
                "measures": "[Grammar]",
                "outcomes": "[1 to 5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Value": "1 to 5"
                },
                "measures": "[Grammar]",
                "outcomes": "[1 to 5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Value": "1 to 5"
                },
                "measures": "[Grammar]",
                "outcomes": "[1 to 5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Value": "1 to 5"
                },
                "measures": "[Grammar]",
                "outcomes": "[1 to 5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Value": "1 to 5"
                },
                "measures": "[Grammar]",
                "outcomes": "[1 to 5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Value": "1 to 5"
                },
                "measures": "[Grammar]",
                "outcomes": "[1 to 5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Value": "1 to 5"
                },
                "measures": "[Grammar]",
                "outcomes": "[1 to 5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Value": "1 to 5"
                },
                "measures": "[Grammar]",
                "outcomes": "[1 to 5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Value": "1 to 5"
                },
                "measures": "[Grammar]",
                "outcomes": "[1 to 5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human",
                    "Number of participants": "3"
                },
                "measures": "[Human]",
                "outcomes": "[3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human",
                    "Number of participants": "3"
                },
                "measures": "[Human]",
                "outcomes": "[3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human",
                    "Number of participants": "3"
                },
                "measures": "[Human]",
                "outcomes": "[3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human",
                    "Number of participants": "3"
                },
                "measures": "[Human]",
                "outcomes": "[3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human",
                    "Number of participants": "3"
                },
                "measures": "[Human]",
                "outcomes": "[3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human",
                    "Number of participants": "3"
                },
                "measures": "[Human]",
                "outcomes": "[3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human",
                    "Number of participants": "3"
                },
                "measures": "[Human]",
                "outcomes": "[3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human",
                    "Number of participants": "3"
                },
                "measures": "[Human]",
                "outcomes": "[3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human",
                    "Number of participants": "3"
                },
                "measures": "[Human]",
                "outcomes": "[3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human",
                    "Number of participants": "3"
                },
                "measures": "[Human]",
                "outcomes": "[3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human",
                    "Number of participants": "3"
                },
                "measures": "[Human]",
                "outcomes": "[3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human",
                    "Number of participants": "3"
                },
                "measures": "[Human]",
                "outcomes": "[3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "LCR",
                    "Voting method": "Majority vote"
                },
                "measures": "[LCR]",
                "outcomes": "[Majority vote]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "LCR",
                    "Voting method": "Majority vote"
                },
                "measures": "[LCR]",
                "outcomes": "[Majority vote]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "LCR",
                    "Voting method": "Majority vote"
                },
                "measures": "[LCR]",
                "outcomes": "[Majority vote]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "LCR",
                    "Voting method": "Majority vote"
                },
                "measures": "[LCR]",
                "outcomes": "[Majority vote]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "LCR",
                    "Voting method": "Majority vote"
                },
                "measures": "[LCR]",
                "outcomes": "[Majority vote]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "LCR",
                    "Voting method": "Majority vote"
                },
                "measures": "[LCR]",
                "outcomes": "[Majority vote]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "LCR",
                    "Voting method": "Majority vote"
                },
                "measures": "[LCR]",
                "outcomes": "[Majority vote]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "LCR",
                    "Voting method": "Majority vote"
                },
                "measures": "[LCR]",
                "outcomes": "[Majority vote]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "LCR",
                    "Voting method": "Majority vote"
                },
                "measures": "[LCR]",
                "outcomes": "[Majority vote]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "LCR",
                    "Voting method": "Majority vote"
                },
                "measures": "[LCR]",
                "outcomes": "[Majority vote]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "LCR",
                    "Voting method": "Majority vote"
                },
                "measures": "[LCR]",
                "outcomes": "[Majority vote]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "LCR",
                    "Voting method": "Majority vote"
                },
                "measures": "[LCR]",
                "outcomes": "[Majority vote]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Grammar",
                    "Scoring method": "Average"
                },
                "measures": "[Grammar]",
                "outcomes": "[Average]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Grammar",
                    "Scoring method": "Average"
                },
                "measures": "[Grammar]",
                "outcomes": "[Average]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Grammar",
                    "Scoring method": "Average"
                },
                "measures": "[Grammar]",
                "outcomes": "[Average]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Grammar",
                    "Scoring method": "Average"
                },
                "measures": "[Grammar]",
                "outcomes": "[Average]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Grammar",
                    "Scoring method": "Average"
                },
                "measures": "[Grammar]",
                "outcomes": "[Average]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Grammar",
                    "Scoring method": "Average"
                },
                "measures": "[Grammar]",
                "outcomes": "[Average]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Grammar",
                    "Scoring method": "Average"
                },
                "measures": "[Grammar]",
                "outcomes": "[Average]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Grammar",
                    "Scoring method": "Average"
                },
                "measures": "[Grammar]",
                "outcomes": "[Average]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Grammar",
                    "Scoring method": "Average"
                },
                "measures": "[Grammar]",
                "outcomes": "[Average]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Grammar",
                    "Scoring method": "Average"
                },
                "measures": "[Grammar]",
                "outcomes": "[Average]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Grammar",
                    "Scoring method": "Average"
                },
                "measures": "[Grammar]",
                "outcomes": "[Average]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Grammar",
                    "Scoring method": "Average"
                },
                "measures": "[Grammar]",
                "outcomes": "[Average]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Description": "100 examples",
                    "Sampling method": "Random"
                },
                "measures": "[Description]",
                "outcomes": "[100 examples, Random]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Description": "100 examples",
                    "Sampling method": "Random"
                },
                "measures": "[Description]",
                "outcomes": "[100 examples, Random]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Description": "100 examples",
                    "Sampling method": "Random"
                },
                "measures": "[Description]",
                "outcomes": "[100 examples, Random]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Description": "100 examples",
                    "Sampling method": "Random"
                },
                "measures": "[Description]",
                "outcomes": "[100 examples, Random]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Description": "100 examples",
                    "Sampling method": "Random"
                },
                "measures": "[Description]",
                "outcomes": "[100 examples, Random]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Description": "100 examples",
                    "Sampling method": "Random"
                },
                "measures": "[Description]",
                "outcomes": "[100 examples, Random]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Description": "100 examples",
                    "Sampling method": "Random"
                },
                "measures": "[Description]",
                "outcomes": "[100 examples, Random]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Description": "100 examples",
                    "Sampling method": "Random"
                },
                "measures": "[Description]",
                "outcomes": "[100 examples, Random]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Description": "100 examples",
                    "Sampling method": "Random"
                },
                "measures": "[Description]",
                "outcomes": "[100 examples, Random]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Description": "100 examples",
                    "Sampling method": "Random"
                },
                "measures": "[Description]",
                "outcomes": "[100 examples, Random]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Description": "100 examples",
                    "Sampling method": "Random"
                },
                "measures": "[Description]",
                "outcomes": "[100 examples, Random]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Description": "100 examples",
                    "Sampling method": "Random"
                },
                "measures": "[Description]",
                "outcomes": "[100 examples, Random]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Table": "6",
                    "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                },
                "measures": "[Table]",
                "outcomes": "[6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Table": "6",
                    "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                },
                "measures": "[Table]",
                "outcomes": "[6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Table": "6",
                    "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                },
                "measures": "[Table]",
                "outcomes": "[6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Table": "6",
                    "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                },
                "measures": "[Table]",
                "outcomes": "[6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Table": "6",
                    "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                },
                "measures": "[Table]",
                "outcomes": "[6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Table": "6",
                    "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                },
                "measures": "[Table]",
                "outcomes": "[6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Table": "6",
                    "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                },
                "measures": "[Table]",
                "outcomes": "[6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Table": "6",
                    "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                },
                "measures": "[Table]",
                "outcomes": "[6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Table": "6",
                    "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                },
                "measures": "[Table]",
                "outcomes": "[6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Table": "6",
                    "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                },
                "measures": "[Table]",
                "outcomes": "[6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Table": "6",
                    "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                },
                "measures": "[Table]",
                "outcomes": "[6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Table": "6",
                    "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                },
                "measures": "[Table]",
                "outcomes": "[6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        }
    ],
    "total_extracted_claims": 11,
    "total_ground_truth_claims": 12,
    "number_of_matches": 0
}