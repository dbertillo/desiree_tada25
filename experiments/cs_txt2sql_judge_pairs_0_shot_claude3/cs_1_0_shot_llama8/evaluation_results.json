{
    "2306.00739_4": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "0-shot",
                        "EX": "81.2",
                        "TS": "76.0"
                    },
                    "measures": "[Prompt design, Concise]",
                    "outcomes": [
                        "[Adaptation setting, 0-shot]",
                        "[EX, 81.2]",
                        "[TS, 76.0]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "EX"
                    ],
                    "outcomes": [
                        "81.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "0-shot",
                        "EX": "78.5",
                        "TS": "70.9"
                    },
                    "measures": "[Prompt design, Verbose]",
                    "outcomes": [
                        "[Adaptation setting, 0-shot]",
                        "[EX, 78.5]",
                        "[TS, 70.9]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "0-shot",
                        "EX": "78.5",
                        "TS": "70.9"
                    },
                    "measures": "[Prompt design, Verbose]",
                    "outcomes": [
                        "[Adaptation setting, 0-shot]",
                        "[EX, 78.5]",
                        "[TS, 70.9]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "EX"
                    ],
                    "outcomes": [
                        "78.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "4-shot",
                        "EX": "82.7",
                        "TS": "77.3"
                    },
                    "measures": "[Prompt design, Concise]",
                    "outcomes": [
                        "[Adaptation setting, 4-shot]",
                        "[EX, 82.7]",
                        "[TS, 77.3]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "4-shot",
                        "EX": "82.7",
                        "TS": "77.3"
                    },
                    "measures": "[Prompt design, Concise]",
                    "outcomes": [
                        "[Adaptation setting, 4-shot]",
                        "[EX, 82.7]",
                        "[TS, 77.3]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "4-shot",
                        "EX": "82.7",
                        "TS": "77.3"
                    },
                    "measures": "[Prompt design, Concise]",
                    "outcomes": [
                        "[Adaptation setting, 4-shot]",
                        "[EX, 82.7]",
                        "[TS, 77.3]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "EX"
                    ],
                    "outcomes": [
                        "82.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "4-shot",
                        "EX": "81.3",
                        "TS": "73.7"
                    },
                    "measures": "[Prompt design, Verbose]",
                    "outcomes": [
                        "[Adaptation setting, 4-shot]",
                        "[EX, 81.3]",
                        "[TS, 73.7]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "4-shot",
                        "EX": "81.3",
                        "TS": "73.7"
                    },
                    "measures": "[Prompt design, Verbose]",
                    "outcomes": [
                        "[Adaptation setting, 4-shot]",
                        "[EX, 81.3]",
                        "[TS, 73.7]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "4-shot",
                        "EX": "81.3",
                        "TS": "73.7"
                    },
                    "measures": "[Prompt design, Verbose]",
                    "outcomes": [
                        "[Adaptation setting, 4-shot]",
                        "[EX, 81.3]",
                        "[TS, 73.7]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "4-shot",
                        "EX": "81.3",
                        "TS": "73.7"
                    },
                    "measures": "[Prompt design, Verbose]",
                    "outcomes": [
                        "[Adaptation setting, 4-shot]",
                        "[EX, 81.3]",
                        "[TS, 73.7]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "EX"
                    ],
                    "outcomes": [
                        "81.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "few-shot"
                    },
                    "measures": "[Prompt design, Concise]",
                    "outcomes": "[Adaptation setting, few-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "few-shot"
                    },
                    "measures": "[Prompt design, Concise]",
                    "outcomes": "[Adaptation setting, few-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "few-shot"
                    },
                    "measures": "[Prompt design, Concise]",
                    "outcomes": "[Adaptation setting, few-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "few-shot"
                    },
                    "measures": "[Prompt design, Concise]",
                    "outcomes": "[Adaptation setting, few-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "73.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "zero-shot"
                    },
                    "measures": "[Prompt design, Verbose]",
                    "outcomes": "[Adaptation setting, zero-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "zero-shot"
                    },
                    "measures": "[Prompt design, Verbose]",
                    "outcomes": "[Adaptation setting, zero-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "zero-shot"
                    },
                    "measures": "[Prompt design, Verbose]",
                    "outcomes": "[Adaptation setting, zero-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "zero-shot"
                    },
                    "measures": "[Prompt design, Verbose]",
                    "outcomes": "[Adaptation setting, zero-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "73.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "zero-shot"
                    },
                    "measures": "[Prompt design, Concise]",
                    "outcomes": "[Adaptation setting, zero-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "zero-shot"
                    },
                    "measures": "[Prompt design, Concise]",
                    "outcomes": "[Adaptation setting, zero-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "zero-shot"
                    },
                    "measures": "[Prompt design, Concise]",
                    "outcomes": "[Adaptation setting, zero-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "zero-shot"
                    },
                    "measures": "[Prompt design, Concise]",
                    "outcomes": "[Adaptation setting, zero-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "73.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "few-shot",
                        "Adaptation setting": "zero-shot"
                    },
                    "measures": "[Prompt design, few-shot]",
                    "outcomes": "[Adaptation setting, zero-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "few-shot",
                        "Adaptation setting": "zero-shot"
                    },
                    "measures": "[Prompt design, few-shot]",
                    "outcomes": "[Adaptation setting, zero-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "few-shot",
                        "Adaptation setting": "zero-shot"
                    },
                    "measures": "[Prompt design, few-shot]",
                    "outcomes": "[Adaptation setting, zero-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "few-shot",
                        "Adaptation setting": "zero-shot"
                    },
                    "measures": "[Prompt design, few-shot]",
                    "outcomes": "[Adaptation setting, zero-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "73.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "few-shot",
                        "Adaptation setting": "few-shot"
                    },
                    "measures": "[Prompt design, few-shot]",
                    "outcomes": "[Adaptation setting, few-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "few-shot",
                        "Adaptation setting": "few-shot"
                    },
                    "measures": "[Prompt design, few-shot]",
                    "outcomes": "[Adaptation setting, few-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "few-shot",
                        "Adaptation setting": "few-shot"
                    },
                    "measures": "[Prompt design, few-shot]",
                    "outcomes": "[Adaptation setting, few-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Prompt design": "few-shot",
                        "Adaptation setting": "few-shot"
                    },
                    "measures": "[Prompt design, few-shot]",
                    "outcomes": "[Adaptation setting, few-shot]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "73.7"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 9,
        "total_ground_truth_claims": 8,
        "number_of_matches": 4
    },
    "1909.00786_4": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "SQLNet",
                        "author": "Xu et al. (2017)",
                        "dev_set": "10.9",
                        "test_set": "12.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[10.9, 12.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLNet",
                        "Dataset": "Spider",
                        "Citation": "Xu etal. (2017)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "10.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "SQLNet",
                        "author": "Xu et al. (2017)",
                        "dev_set": "10.9",
                        "test_set": "12.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[10.9, 12.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLNet",
                        "Dataset": "Spider",
                        "Citation": "Xu etal. (2017)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "12.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "SyntaxSQLNet",
                        "author": "Yu et al. (2018b)",
                        "dev_set": "18.9",
                        "test_set": "19.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[18.9, 19.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLNet",
                        "Dataset": "Spider",
                        "Citation": "Xu etal. (2017)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "10.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "SyntaxSQLNet",
                        "author": "Yu et al. (2018b)",
                        "dev_set": "18.9",
                        "test_set": "19.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[18.9, 19.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "18.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "SyntaxSQLNet",
                        "author": "Yu et al. (2018b)",
                        "dev_set": "18.9",
                        "test_set": "19.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[18.9, 19.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "19.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+data augmentation",
                        "author": "Yu et al. (2018b)",
                        "dev_set": "24.8",
                        "test_set": "27.2"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[24.8, 27.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLNet",
                        "Dataset": "Spider",
                        "Citation": "Xu etal. (2017)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "10.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+data augmentation",
                        "author": "Yu et al. (2018b)",
                        "dev_set": "24.8",
                        "test_set": "27.2"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[24.8, 27.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "18.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+data augmentation",
                        "author": "Yu et al. (2018b)",
                        "dev_set": "24.8",
                        "test_set": "27.2"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[24.8, 27.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet + data augmentation",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+data augmentation",
                        "author": "Yu et al. (2018b)",
                        "dev_set": "24.8",
                        "test_set": "27.2"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[24.8, 27.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet + data augmentation",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "27.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLNet",
                        "Dataset": "Spider",
                        "Citation": "Xu etal. (2017)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "10.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "18.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet + data augmentation",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Lee (2019)",
                        "Dataset": "Spider",
                        "Citation": "Lee (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "28.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Lee (2019)",
                        "Dataset": "Spider",
                        "Citation": "Lee (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "GNN",
                        "Dataset": "Spider",
                        "Citation": "Bogin etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "40.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "GNN",
                        "Dataset": "Spider",
                        "Citation": "Bogin etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "39.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "53.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "46.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet (BERT)",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "61.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet (BERT)",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "54.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours",
                        "Dataset": "Spider",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "36.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours",
                        "Dataset": "Spider",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "32.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours + utterance-table BERT Embedding",
                        "Dataset": "Spider",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "57.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Lee (2019)",
                        "dev_set": "28.5",
                        "test_set": "24.3"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[28.5, 24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours + utterance-table BERT Embedding",
                        "Dataset": "Spider",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLNet",
                        "Dataset": "Spider",
                        "Citation": "Xu etal. (2017)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "10.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "18.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet + data augmentation",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Lee (2019)",
                        "Dataset": "Spider",
                        "Citation": "Lee (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "28.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Lee (2019)",
                        "Dataset": "Spider",
                        "Citation": "Lee (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "GNN",
                        "Dataset": "Spider",
                        "Citation": "Bogin etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "40.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "GNN",
                        "Dataset": "Spider",
                        "Citation": "Bogin etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "39.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "53.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "46.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet (BERT)",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "61.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet (BERT)",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "54.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours",
                        "Dataset": "Spider",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "36.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours",
                        "Dataset": "Spider",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "32.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours + utterance-table BERT Embedding",
                        "Dataset": "Spider",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "57.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "GNN",
                        "author": "Bogin et al. (2019)",
                        "dev_set": "40.7",
                        "test_set": "39.4"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[40.7, 39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours + utterance-table BERT Embedding",
                        "Dataset": "Spider",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLNet",
                        "Dataset": "Spider",
                        "Citation": "Xu etal. (2017)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "10.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "18.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet + data augmentation",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Lee (2019)",
                        "Dataset": "Spider",
                        "Citation": "Lee (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "28.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Lee (2019)",
                        "Dataset": "Spider",
                        "Citation": "Lee (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "GNN",
                        "Dataset": "Spider",
                        "Citation": "Bogin etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "40.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "GNN",
                        "Dataset": "Spider",
                        "Citation": "Bogin etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "39.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "53.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "46.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet (BERT)",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "61.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet (BERT)",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "54.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours",
                        "Dataset": "Spider",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "36.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours",
                        "Dataset": "Spider",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "32.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours + utterance-table BERT Embedding",
                        "Dataset": "Spider",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "57.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet",
                        "author": "Guo et al. (2019)",
                        "dev_set": "53.2",
                        "test_set": "46.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[53.2, 46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours + utterance-table BERT Embedding",
                        "Dataset": "Spider",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLNet",
                        "Dataset": "Spider",
                        "Citation": "Xu etal. (2017)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "10.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "18.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet + data augmentation",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Lee (2019)",
                        "Dataset": "Spider",
                        "Citation": "Lee (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "28.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Lee (2019)",
                        "Dataset": "Spider",
                        "Citation": "Lee (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "GNN",
                        "Dataset": "Spider",
                        "Citation": "Bogin etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "40.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "GNN",
                        "Dataset": "Spider",
                        "Citation": "Bogin etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "39.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "53.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "46.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet (BERT)",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "61.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet (BERT)",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "54.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours",
                        "Dataset": "Spider",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "36.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours",
                        "Dataset": "Spider",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "32.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours + utterance-table BERT Embedding",
                        "Dataset": "Spider",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "57.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "IRNet (BERT)",
                        "author": "Guo et al. (2019)",
                        "dev_set": "61.9",
                        "test_set": "54.7"
                    },
                    "measures": "[dataset, model, author]",
                    "outcomes": "[61.9, 54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours + utterance-table BERT Embedding",
                        "Dataset": "Spider",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLNet",
                        "Dataset": "Spider",
                        "Citation": "Xu etal. (2017)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "10.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "18.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet + data augmentation",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Lee (2019)",
                        "Dataset": "Spider",
                        "Citation": "Lee (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "28.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Lee (2019)",
                        "Dataset": "Spider",
                        "Citation": "Lee (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "GNN",
                        "Dataset": "Spider",
                        "Citation": "Bogin etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "40.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "GNN",
                        "Dataset": "Spider",
                        "Citation": "Bogin etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "39.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "53.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "46.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet (BERT)",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "61.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet (BERT)",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "54.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours",
                        "Dataset": "Spider",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "36.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours",
                        "Dataset": "Spider",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "32.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours + utterance-table BERT Embedding",
                        "Dataset": "Spider",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "57.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "Ours",
                        "dev_set": "36.4",
                        "test_set": "32.9"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[36.4, 32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours + utterance-table BERT Embedding",
                        "Dataset": "Spider",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLNet",
                        "Dataset": "Spider",
                        "Citation": "Xu etal. (2017)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "10.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "18.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet + data augmentation",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Lee (2019)",
                        "Dataset": "Spider",
                        "Citation": "Lee (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "28.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Lee (2019)",
                        "Dataset": "Spider",
                        "Citation": "Lee (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "GNN",
                        "Dataset": "Spider",
                        "Citation": "Bogin etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "40.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "GNN",
                        "Dataset": "Spider",
                        "Citation": "Bogin etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "39.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "53.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "46.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet (BERT)",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "61.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet (BERT)",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "54.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours",
                        "Dataset": "Spider",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "36.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours",
                        "Dataset": "Spider",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "32.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours + utterance-table BERT Embedding",
                        "Dataset": "Spider",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "57.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider",
                        "model": "+ utterance-table BERT Embedding",
                        "dev_set": "57.6",
                        "test_set": "53.4"
                    },
                    "measures": "[dataset, model]",
                    "outcomes": "[57.6, 53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours + utterance-table BERT Embedding",
                        "Dataset": "Spider",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 9,
        "total_ground_truth_claims": 18,
        "number_of_matches": 4
    },
    "2104.04689_2": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "54.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "35.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "28.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "50.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "78.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "29.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "58.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "85.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "56.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "32.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "65.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "74.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "59.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "41.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "69.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "74.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "57.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "46.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "78.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "61.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "45.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "72.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "54.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "35.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "28.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "50.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "78.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "29.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "58.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "85.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "56.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "32.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "65.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "74.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "59.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "41.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "69.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "74.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "57.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "46.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "78.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "61.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "45.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "72.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "28.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "50.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "78.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "29.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "58.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "85.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "56.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "32.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "65.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "74.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "59.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "41.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "69.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "74.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "57.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "46.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "78.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "61.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "45.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "72.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Easy",
                        "Accuracy": "70.4%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "28.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "50.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "78.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "29.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "58.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "85.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "56.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "32.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "65.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "74.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "59.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "41.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "69.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "74.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "57.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "46.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "78.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "61.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "45.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Medium",
                        "Accuracy": "54.1%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "72.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "28.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "50.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "78.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "29.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "58.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "85.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "56.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "32.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "65.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "74.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "59.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "41.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "69.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "74.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "57.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "46.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "78.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "61.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "45.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Hard",
                        "Accuracy": "35.6%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "72.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "28.2%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "28.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN Kelkar et al. (2020)",
                        "Hardness Level": "All",
                        "Accuracy": "50.7%",
                        "Reference": "Kelkar et al. (2020)"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Reference, Kelkar et al. (2020)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "50.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Accuracy": "78.9%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "78.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Medium",
                        "Accuracy": "63.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Hard",
                        "Accuracy": "46.6%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "29.8%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "29.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Accuracy": "58.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "58.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN+RAT",
                        "Hardness Level": "Easy",
                        "Accuracy": "85.0%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "85.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN+RAT",
                        "Hardness Level": "Medium",
                        "Accuracy": "70.9%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN+RAT",
                        "Hardness Level": "Hard",
                        "Accuracy": "56.3%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "56.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN+RAT",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "32.7%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "32.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN+RAT",
                        "Hardness Level": "All",
                        "Accuracy": "65.6%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "65.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "GPNN",
                        "Hardness Level": "Easy",
                        "Accuracy": "87.5%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "GPNN",
                        "Hardness Level": "Medium",
                        "Accuracy": "74.9%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "74.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "GPNN",
                        "Hardness Level": "Hard",
                        "Accuracy": "59.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "59.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "GPNN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "41.6%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "41.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "GPNN",
                        "Hardness Level": "All",
                        "Accuracy": "69.9%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "69.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "RATSQL",
                        "Hardness Level": "Easy",
                        "Accuracy": "87.1%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "RATSQL",
                        "Hardness Level": "Medium",
                        "Accuracy": "74.9%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "74.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "RATSQL",
                        "Hardness Level": "Hard",
                        "Accuracy": "57.5%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "57.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "RATSQL",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "46.4%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "46.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "RATSQL",
                        "Hardness Level": "All",
                        "Accuracy": "70.2%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "ShadowGNN",
                        "Hardness Level": "Easy",
                        "Accuracy": "87.5%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "ShadowGNN",
                        "Hardness Level": "Medium",
                        "Accuracy": "78.0%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "78.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "ShadowGNN",
                        "Hardness Level": "Hard",
                        "Accuracy": "61.5%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "61.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "ShadowGNN",
                        "Hardness Level": "Extra Hard",
                        "Accuracy": "45.8%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "45.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "ShadowGNN",
                        "Hardness Level": "All",
                        "Accuracy": "72.3%",
                        "Implemented By": "Us"
                    },
                    "measures": "[Model, Approach]",
                    "outcomes": "[Implemented By, Us]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "72.3"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 35,
        "total_ground_truth_claims": 30,
        "number_of_matches": 30
    },
    "2212.09278_5": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Difficulty": "Easy",
                        "Accuracy": "68.8",
                        "Metric": "Accuracy",
                        "Outcome": "68.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "68.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Difficulty": "Medium",
                        "Accuracy": "40.6",
                        "Metric": "Accuracy",
                        "Outcome": "40.6"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "40.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Difficulty": "Hard",
                        "Accuracy": "26.9",
                        "Metric": "Accuracy",
                        "Outcome": "26.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "26.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Difficulty": "Extra",
                        "Accuracy": "12.8",
                        "Metric": "Accuracy",
                        "Outcome": "12.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "12.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Difficulty": "Easy",
                        "Accuracy": "70.9",
                        "Metric": "Accuracy",
                        "Outcome": "70.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Difficulty": "Medium",
                        "Accuracy": "45.4",
                        "Metric": "Accuracy",
                        "Outcome": "45.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "45.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Difficulty": "Hard",
                        "Accuracy": "29.0",
                        "Metric": "Accuracy",
                        "Outcome": "29.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "29.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Difficulty": "Extra",
                        "Accuracy": "18.8",
                        "Metric": "Accuracy",
                        "Outcome": "18.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "18.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Difficulty": "Easy",
                        "Accuracy": "75.5",
                        "Metric": "Accuracy",
                        "Outcome": "75.5"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "75.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Difficulty": "Medium",
                        "Accuracy": "51.5",
                        "Metric": "Accuracy",
                        "Outcome": "51.5"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "51.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Difficulty": "Hard",
                        "Accuracy": "35.2",
                        "Metric": "Accuracy",
                        "Outcome": "35.2"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "35.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Difficulty": "Extra",
                        "Accuracy": "21.8",
                        "Metric": "Accuracy",
                        "Outcome": "21.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "21.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Difficulty": "Easy",
                        "Accuracy": "80.7",
                        "Metric": "Accuracy",
                        "Outcome": "80.7"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "80.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Difficulty": "Medium",
                        "Accuracy": "68.3",
                        "Metric": "Accuracy",
                        "Outcome": "68.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "68.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Difficulty": "Hard",
                        "Accuracy": "46.2",
                        "Metric": "Accuracy",
                        "Outcome": "46.2"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "46.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Difficulty": "Extra",
                        "Accuracy": "43.3",
                        "Metric": "Accuracy",
                        "Outcome": "43.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "43.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Difficulty": "Easy",
                        "Accuracy": "81.8",
                        "Metric": "Accuracy",
                        "Outcome": "81.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "81.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Difficulty": "Medium",
                        "Accuracy": "66.7",
                        "Metric": "Accuracy",
                        "Outcome": "66.7"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Difficulty": "Hard",
                        "Accuracy": "44.8",
                        "Metric": "Accuracy",
                        "Outcome": "44.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "44.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Difficulty": "Extra",
                        "Accuracy": "41.8",
                        "Metric": "Accuracy",
                        "Outcome": "41.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "41.8"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 33,
        "total_ground_truth_claims": 20,
        "number_of_matches": 20
    },
    "2305.16253_7": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "42.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "75.1",
                        "RESDSQL": "77.9",
                        "NatSQL": "71.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "42.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "72.2",
                        "RESDSQL": "75.7",
                        "NatSQL": "72.5"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "42.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "73.7",
                        "RESDSQL": "77.8",
                        "NatSQL": "73.9"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "42.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Re-ranker": "N/A",
                        "SmBoP": "80.5",
                        "RESDSQL": "86.6",
                        "NatSQL": "81.1"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "42.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "0.3"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "42.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "BRIDGE v2": "21.8"
                    },
                    "measures": "[Table, 4]",
                    "outcomes": [
                        "[Caption, Execution accuracy with re-ranking using the CodeBERT + GAT model. RR: Re-ranking all beams; ED+RR: Re-ranking beams after error detection.]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "42.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "bottom-up nature of its decoder"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "42.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "SmBoP": "less consistent on the test split"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "42.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "State-of-the-art text-to-SQL parsers": "autoregressive decoders"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "42.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "CodeBERT+GAT": "good generalization to unseen datasets"
                    },
                    "measures": "[Section, 3.2]",
                    "outcomes": [
                        "[Section, 4.2.2]",
                        "[Dataset, KaggleDBQA Lee etal. (2021)]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 10,
        "total_ground_truth_claims": 28,
        "number_of_matches": 0
    },
    "2211.06193_1": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Failure Categories": "Incomplete SQL",
                        "Percentage": "6.2%",
                        "Table": "Table 1",
                        "Caption": "Error Analysis on Spider dev based on T5+3B with constrained decoding"
                    },
                    "measures": "[Failure Categories, Percentage]",
                    "outcomes": "[Incomplete SQL, 6.2%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "Incomplete SQL"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "6.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Failure Categories": "False Negatives",
                        "Percentage": "22.4%",
                        "Table": "Table 1",
                        "Caption": "Error Analysis on Spider dev based on T5+3B with constrained decoding"
                    },
                    "measures": "[Failure Categories, Percentage]",
                    "outcomes": "[False Negatives, 22.4%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "False Negatives"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "22.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Failure Categories": "Foreign Keys",
                        "Percentage": "19.6%",
                        "Table": "Table 1",
                        "Caption": "Error Analysis on Spider dev based on T5+3B with constrained decoding"
                    },
                    "measures": "[Failure Categories, Percentage]",
                    "outcomes": "[Foreign Keys, 19.6%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "Foreign Keys"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "19.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Failure Categories": "Logical Errors",
                        "Percentage": "2.8%",
                        "Table": "Table 1",
                        "Caption": "Error Analysis on Spider dev based on T5+3B with constrained decoding"
                    },
                    "measures": "[Failure Categories, Percentage]",
                    "outcomes": "[Logical Errors, 2.8%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "Logical Errors"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "2.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Failure Categories": "DK - Incorrect AGG",
                        "Percentage": "17.2%",
                        "Table": "Table 1",
                        "Caption": "Error Analysis on Spider dev based on T5+3B with constrained decoding"
                    },
                    "measures": "[Failure Categories, Percentage]",
                    "outcomes": "[DK - Incorrect AGG, 17.2%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "DK - Incorrect AGG"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "17.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Failure Categories": "DK - Incorrect Table",
                        "Percentage": "3.8%",
                        "Table": "Table 1",
                        "Caption": "Error Analysis on Spider dev based on T5+3B with constrained decoding"
                    },
                    "measures": "[Failure Categories, Percentage]",
                    "outcomes": "[DK - Incorrect Table, 3.8%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "DK - Incorrect Table"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "3.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Failure Categories": "DK - Incorrect Column",
                        "Percentage": "13.4%",
                        "Table": "Table 1",
                        "Caption": "Error Analysis on Spider dev based on T5+3B with constrained decoding"
                    },
                    "measures": "[Failure Categories, Percentage]",
                    "outcomes": "[DK - Incorrect Column, 13.4%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "DK - Incorrect Column"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "13.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Failure Categories": "DK - Incorrect Value",
                        "Percentage": "3.8%",
                        "Table": "Table 1",
                        "Caption": "Error Analysis on Spider dev based on T5+3B with constrained decoding"
                    },
                    "measures": "[Failure Categories, Percentage]",
                    "outcomes": "[DK - Incorrect Value, 3.8%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "DK - Incorrect Value"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "3.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Failure Categories": "DK - Complex",
                        "Percentage": "11.0%",
                        "Table": "Table 1",
                        "Caption": "Error Analysis on Spider dev based on T5+3B with constrained decoding"
                    },
                    "measures": "[Failure Categories, Percentage]",
                    "outcomes": "[DK - Complex, 11.0%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "DK - Complex"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "11.0"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 10,
        "total_ground_truth_claims": 9,
        "number_of_matches": 9
    },
    "2205.02054_4": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.4%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 93.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.4%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 93.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.4%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 93.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.4%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 93.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.4%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 93.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "86.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.4%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 93.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.4%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 93.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.4%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 93.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.1%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 92.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.1%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 92.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.1%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 92.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.1%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 92.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.1%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 92.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "86.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.1%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 92.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.1%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 92.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-APP",
                        "deviation": "<= 2",
                        "similarity": "94.1%"
                    },
                    "measures": "[dataset, CG-SUB]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 92.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-SUB",
                        "deviation": "<= 2",
                        "similarity": "90.4%"
                    },
                    "measures": "[dataset, CG-APP]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 86.0%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-SUB",
                        "deviation": "<= 2",
                        "similarity": "90.4%"
                    },
                    "measures": "[dataset, CG-APP]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 86.0%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-SUB",
                        "deviation": "<= 2",
                        "similarity": "90.4%"
                    },
                    "measures": "[dataset, CG-APP]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 86.0%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-SUB",
                        "deviation": "<= 2",
                        "similarity": "90.4%"
                    },
                    "measures": "[dataset, CG-APP]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 86.0%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-SUB",
                        "deviation": "<= 2",
                        "similarity": "90.4%"
                    },
                    "measures": "[dataset, CG-APP]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 86.0%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "86.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-SUB",
                        "deviation": "<= 2",
                        "similarity": "92.6%"
                    },
                    "measures": "[dataset, CG-APP]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 88.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-SUB",
                        "deviation": "<= 2",
                        "similarity": "92.6%"
                    },
                    "measures": "[dataset, CG-APP]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 88.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-SUB",
                        "deviation": "<= 2",
                        "similarity": "92.6%"
                    },
                    "measures": "[dataset, CG-APP]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 88.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-SUB",
                        "deviation": "<= 2",
                        "similarity": "92.6%"
                    },
                    "measures": "[dataset, CG-APP]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 88.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-SUB",
                        "deviation": "<= 2",
                        "similarity": "92.6%"
                    },
                    "measures": "[dataset, CG-APP]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 88.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "CG-SUB",
                        "deviation": "<= 2",
                        "similarity": "92.6%"
                    },
                    "measures": "[dataset, CG-APP]",
                    "outcomes": [
                        "[deviation, <= 1]",
                        "[similarity, 88.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "consistent"
                    },
                    "measures": "[algorithm, refined]",
                    "outcomes": "[algorithm, consistent]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "consistent"
                    },
                    "measures": "[algorithm, refined]",
                    "outcomes": "[algorithm, consistent]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "consistent"
                    },
                    "measures": "[algorithm, refined]",
                    "outcomes": "[algorithm, consistent]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "consistent"
                    },
                    "measures": "[algorithm, refined]",
                    "outcomes": "[algorithm, consistent]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "consistent"
                    },
                    "measures": "[algorithm, refined]",
                    "outcomes": "[algorithm, consistent]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "consistent"
                    },
                    "measures": "[algorithm, refined]",
                    "outcomes": "[algorithm, consistent]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "sentence complexity": "CG-SUB"
                    },
                    "measures": "[sentence complexity, CG-APP]",
                    "outcomes": "[sentence complexity, CG-SUB]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "sentence complexity": "CG-SUB"
                    },
                    "measures": "[sentence complexity, CG-APP]",
                    "outcomes": "[sentence complexity, CG-SUB]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "sentence complexity": "CG-SUB"
                    },
                    "measures": "[sentence complexity, CG-APP]",
                    "outcomes": "[sentence complexity, CG-SUB]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "sentence complexity": "CG-SUB"
                    },
                    "measures": "[sentence complexity, CG-APP]",
                    "outcomes": "[sentence complexity, CG-SUB]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "sentence complexity": "CG-SUB"
                    },
                    "measures": "[sentence complexity, CG-APP]",
                    "outcomes": "[sentence complexity, CG-SUB]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "sentence complexity": "CG-SUB"
                    },
                    "measures": "[sentence complexity, CG-APP]",
                    "outcomes": "[sentence complexity, CG-SUB]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stable",
                        "dataset": "other text-to-SQL"
                    },
                    "measures": "[algorithm, stable]",
                    "outcomes": "[dataset, other text-to-SQL]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stable",
                        "dataset": "other text-to-SQL"
                    },
                    "measures": "[algorithm, stable]",
                    "outcomes": "[dataset, other text-to-SQL]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stable",
                        "dataset": "other text-to-SQL"
                    },
                    "measures": "[algorithm, stable]",
                    "outcomes": "[dataset, other text-to-SQL]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stable",
                        "dataset": "other text-to-SQL"
                    },
                    "measures": "[algorithm, stable]",
                    "outcomes": "[dataset, other text-to-SQL]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stable",
                        "dataset": "other text-to-SQL"
                    },
                    "measures": "[algorithm, stable]",
                    "outcomes": "[dataset, other text-to-SQL]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stable",
                        "dataset": "other text-to-SQL"
                    },
                    "measures": "[algorithm, stable]",
                    "outcomes": "[dataset, other text-to-SQL]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs",
                        "domain": "unseen"
                    },
                    "measures": "[algorithm, performs]",
                    "outcomes": "[domain, unseen]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs",
                        "domain": "unseen"
                    },
                    "measures": "[algorithm, performs]",
                    "outcomes": "[domain, unseen]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs",
                        "domain": "unseen"
                    },
                    "measures": "[algorithm, performs]",
                    "outcomes": "[domain, unseen]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs",
                        "domain": "unseen"
                    },
                    "measures": "[algorithm, performs]",
                    "outcomes": "[domain, unseen]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs",
                        "domain": "unseen"
                    },
                    "measures": "[algorithm, performs]",
                    "outcomes": "[domain, unseen]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs",
                        "domain": "unseen"
                    },
                    "measures": "[algorithm, performs]",
                    "outcomes": "[domain, unseen]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stably",
                        "dataset": "text-to-SQL"
                    },
                    "measures": "[algorithm, stably]",
                    "outcomes": "[dataset, text-to-SQL]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stably",
                        "dataset": "text-to-SQL"
                    },
                    "measures": "[algorithm, stably]",
                    "outcomes": "[dataset, text-to-SQL]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stably",
                        "dataset": "text-to-SQL"
                    },
                    "measures": "[algorithm, stably]",
                    "outcomes": "[dataset, text-to-SQL]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stably",
                        "dataset": "text-to-SQL"
                    },
                    "measures": "[algorithm, stably]",
                    "outcomes": "[dataset, text-to-SQL]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stably",
                        "dataset": "text-to-SQL"
                    },
                    "measures": "[algorithm, stably]",
                    "outcomes": "[dataset, text-to-SQL]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stably",
                        "dataset": "text-to-SQL"
                    },
                    "measures": "[algorithm, stably]",
                    "outcomes": "[dataset, text-to-SQL]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "model": "does not require"
                    },
                    "measures": "[model, trained]",
                    "outcomes": "[model, does not require]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "model": "does not require"
                    },
                    "measures": "[model, trained]",
                    "outcomes": "[model, does not require]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "model": "does not require"
                    },
                    "measures": "[model, trained]",
                    "outcomes": "[model, does not require]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "model": "does not require"
                    },
                    "measures": "[model, trained]",
                    "outcomes": "[model, does not require]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "model": "does not require"
                    },
                    "measures": "[model, trained]",
                    "outcomes": "[model, does not require]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "model": "does not require"
                    },
                    "measures": "[model, trained]",
                    "outcomes": "[model, does not require]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "sentence": "not complex",
                        "algorithm": "can be used"
                    },
                    "measures": "[sentence, not complex]",
                    "outcomes": "[algorithm, can be used]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "sentence": "not complex",
                        "algorithm": "can be used"
                    },
                    "measures": "[sentence, not complex]",
                    "outcomes": "[algorithm, can be used]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "sentence": "not complex",
                        "algorithm": "can be used"
                    },
                    "measures": "[sentence, not complex]",
                    "outcomes": "[algorithm, can be used]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "sentence": "not complex",
                        "algorithm": "can be used"
                    },
                    "measures": "[sentence, not complex]",
                    "outcomes": "[algorithm, can be used]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "sentence": "not complex",
                        "algorithm": "can be used"
                    },
                    "measures": "[sentence, not complex]",
                    "outcomes": "[algorithm, can be used]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "sentence": "not complex",
                        "algorithm": "can be used"
                    },
                    "measures": "[sentence, not complex]",
                    "outcomes": "[algorithm, can be used]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs consistently"
                    },
                    "measures": "[algorithm, refined on training set]",
                    "outcomes": "[algorithm, performs consistently]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs consistently"
                    },
                    "measures": "[algorithm, refined on training set]",
                    "outcomes": "[algorithm, performs consistently]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs consistently"
                    },
                    "measures": "[algorithm, refined on training set]",
                    "outcomes": "[algorithm, performs consistently]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs consistently"
                    },
                    "measures": "[algorithm, refined on training set]",
                    "outcomes": "[algorithm, performs consistently]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs consistently"
                    },
                    "measures": "[algorithm, refined on training set]",
                    "outcomes": "[algorithm, performs consistently]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs consistently"
                    },
                    "measures": "[algorithm, refined on training set]",
                    "outcomes": "[algorithm, performs consistently]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider-CG"
                    },
                    "measures": "[dataset, Spider-SS]",
                    "outcomes": "[dataset, Spider-CG]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider-CG"
                    },
                    "measures": "[dataset, Spider-SS]",
                    "outcomes": "[dataset, Spider-CG]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider-CG"
                    },
                    "measures": "[dataset, Spider-SS]",
                    "outcomes": "[dataset, Spider-CG]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider-CG"
                    },
                    "measures": "[dataset, Spider-SS]",
                    "outcomes": "[dataset, Spider-CG]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider-CG"
                    },
                    "measures": "[dataset, Spider-SS]",
                    "outcomes": "[dataset, Spider-CG]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Spider-CG"
                    },
                    "measures": "[dataset, Spider-SS]",
                    "outcomes": "[dataset, Spider-CG]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "deviation"
                    },
                    "measures": "[algorithm, split algorithm]",
                    "outcomes": "[algorithm, deviation]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "deviation"
                    },
                    "measures": "[algorithm, split algorithm]",
                    "outcomes": "[algorithm, deviation]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "deviation"
                    },
                    "measures": "[algorithm, split algorithm]",
                    "outcomes": "[algorithm, deviation]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "deviation"
                    },
                    "measures": "[algorithm, split algorithm]",
                    "outcomes": "[algorithm, deviation]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "deviation"
                    },
                    "measures": "[algorithm, split algorithm]",
                    "outcomes": "[algorithm, deviation]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "deviation"
                    },
                    "measures": "[algorithm, split algorithm]",
                    "outcomes": "[algorithm, deviation]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "good enough"
                    },
                    "measures": "[algorithm, split results]",
                    "outcomes": "[algorithm, good enough]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "good enough"
                    },
                    "measures": "[algorithm, split results]",
                    "outcomes": "[algorithm, good enough]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "good enough"
                    },
                    "measures": "[algorithm, split results]",
                    "outcomes": "[algorithm, good enough]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "good enough"
                    },
                    "measures": "[algorithm, split results]",
                    "outcomes": "[algorithm, good enough]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "good enough"
                    },
                    "measures": "[algorithm, split results]",
                    "outcomes": "[algorithm, good enough]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "good enough"
                    },
                    "measures": "[algorithm, split results]",
                    "outcomes": "[algorithm, good enough]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "refined"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, refined]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "refined"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, refined]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "refined"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, refined]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "refined"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, refined]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "refined"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, refined]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "refined"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, refined]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, performs]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, performs]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, performs]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, performs]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, performs]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "performs"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, performs]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "consistently"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, consistently]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "consistently"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, consistently]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "consistently"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, consistently]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "consistently"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, consistently]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "consistently"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, consistently]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "consistently"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, consistently]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stably"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, stably]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stably"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, stably]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stably"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, stably]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stably"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, stably]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stably"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, stably]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "stably"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, stably]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "algorithm": "algorithm"
                    },
                    "measures": "[algorithm, splitting algorithm]",
                    "outcomes": "[algorithm, algorithm]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 113,
        "total_ground_truth_claims": 8,
        "number_of_matches": 2
    },
    "1909.05378_6": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Template",
                        "BLEU": "9.5",
                        "LCR": "41.0",
                        "Grammar": "4.0"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[9.5, 41.0, 4.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Template",
                        "BLEU": "9.5",
                        "LCR": "41.0",
                        "Grammar": "4.0"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[9.5, 41.0, 4.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Automatic",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Template",
                        "BLEU": "9.5",
                        "LCR": "41.0",
                        "Grammar": "4.0"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[9.5, 41.0, 4.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "41.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Template",
                        "BLEU": "9.5",
                        "LCR": "41.0",
                        "Grammar": "4.0"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[9.5, 41.0, 4.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "4.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Template",
                        "BLEU": "9.5",
                        "LCR": "41.0",
                        "Grammar": "4.0"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[9.5, 41.0, 4.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Template",
                        "BLEU": "9.5",
                        "LCR": "41.0",
                        "Grammar": "4.0"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[9.5, 41.0, 4.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Template",
                        "BLEU": "9.5",
                        "LCR": "41.0",
                        "Grammar": "4.0"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[9.5, 41.0, 4.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "27.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Template",
                        "BLEU": "9.5",
                        "LCR": "41.0",
                        "Grammar": "4.0"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[9.5, 41.0, 4.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Template",
                        "BLEU": "9.5",
                        "LCR": "41.0",
                        "Grammar": "4.0"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[9.5, 41.0, 4.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Template",
                        "BLEU": "9.5",
                        "LCR": "41.0",
                        "Grammar": "4.0"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[9.5, 41.0, 4.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Template",
                        "BLEU": "9.5",
                        "LCR": "41.0",
                        "Grammar": "4.0"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[9.5, 41.0, 4.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "35.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Template",
                        "BLEU": "9.5",
                        "LCR": "41.0",
                        "Grammar": "4.0"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[9.5, 41.0, 4.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "BLEU": "15.3",
                        "LCR": "27.0",
                        "Grammar": "3.5"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[15.3, 27.0, 3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "BLEU": "15.3",
                        "LCR": "27.0",
                        "Grammar": "3.5"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[15.3, 27.0, 3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Automatic",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "BLEU": "15.3",
                        "LCR": "27.0",
                        "Grammar": "3.5"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[15.3, 27.0, 3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "41.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "BLEU": "15.3",
                        "LCR": "27.0",
                        "Grammar": "3.5"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[15.3, 27.0, 3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "4.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "BLEU": "15.3",
                        "LCR": "27.0",
                        "Grammar": "3.5"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[15.3, 27.0, 3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "BLEU": "15.3",
                        "LCR": "27.0",
                        "Grammar": "3.5"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[15.3, 27.0, 3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "BLEU": "15.3",
                        "LCR": "27.0",
                        "Grammar": "3.5"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[15.3, 27.0, 3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "27.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "BLEU": "15.3",
                        "LCR": "27.0",
                        "Grammar": "3.5"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[15.3, 27.0, 3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "BLEU": "15.3",
                        "LCR": "27.0",
                        "Grammar": "3.5"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[15.3, 27.0, 3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "BLEU": "15.3",
                        "LCR": "27.0",
                        "Grammar": "3.5"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[15.3, 27.0, 3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "BLEU": "15.3",
                        "LCR": "27.0",
                        "Grammar": "3.5"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[15.3, 27.0, 3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "35.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "BLEU": "15.3",
                        "LCR": "27.0",
                        "Grammar": "3.5"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[15.3, 27.0, 3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "BLEU": "16.4",
                        "LCR": "35.0",
                        "Grammar": "3.6"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[16.4, 35.0, 3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "BLEU": "16.4",
                        "LCR": "35.0",
                        "Grammar": "3.6"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[16.4, 35.0, 3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Automatic",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "BLEU": "16.4",
                        "LCR": "35.0",
                        "Grammar": "3.6"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[16.4, 35.0, 3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "41.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "BLEU": "16.4",
                        "LCR": "35.0",
                        "Grammar": "3.6"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[16.4, 35.0, 3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "4.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "BLEU": "16.4",
                        "LCR": "35.0",
                        "Grammar": "3.6"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[16.4, 35.0, 3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "BLEU": "16.4",
                        "LCR": "35.0",
                        "Grammar": "3.6"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[16.4, 35.0, 3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "BLEU": "16.4",
                        "LCR": "35.0",
                        "Grammar": "3.6"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[16.4, 35.0, 3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "27.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "BLEU": "16.4",
                        "LCR": "35.0",
                        "Grammar": "3.6"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[16.4, 35.0, 3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "BLEU": "16.4",
                        "LCR": "35.0",
                        "Grammar": "3.6"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[16.4, 35.0, 3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "BLEU": "16.4",
                        "LCR": "35.0",
                        "Grammar": "3.6"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[16.4, 35.0, 3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "BLEU": "16.4",
                        "LCR": "35.0",
                        "Grammar": "3.6"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[16.4, 35.0, 3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "35.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "BLEU": "16.4",
                        "LCR": "35.0",
                        "Grammar": "3.6"
                    },
                    "measures": "[BLEU, LCR, Grammar]",
                    "outcomes": "[16.4, 35.0, 3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Value": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU]",
                    "outcomes": "[Papineni et al. (2002)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Value": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU]",
                    "outcomes": "[Papineni et al. (2002)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Automatic",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Value": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU]",
                    "outcomes": "[Papineni et al. (2002)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "41.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Value": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU]",
                    "outcomes": "[Papineni et al. (2002)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "4.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Value": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU]",
                    "outcomes": "[Papineni et al. (2002)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Value": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU]",
                    "outcomes": "[Papineni et al. (2002)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Value": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU]",
                    "outcomes": "[Papineni et al. (2002)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "27.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Value": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU]",
                    "outcomes": "[Papineni et al. (2002)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Value": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU]",
                    "outcomes": "[Papineni et al. (2002)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Value": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU]",
                    "outcomes": "[Papineni et al. (2002)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Value": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU]",
                    "outcomes": "[Papineni et al. (2002)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "35.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Value": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU]",
                    "outcomes": "[Papineni et al. (2002)]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Value": "0 or 1"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[0 or 1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Value": "0 or 1"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[0 or 1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Automatic",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Value": "0 or 1"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[0 or 1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "41.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Value": "0 or 1"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[0 or 1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "4.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Value": "0 or 1"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[0 or 1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Value": "0 or 1"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[0 or 1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Value": "0 or 1"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[0 or 1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "27.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Value": "0 or 1"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[0 or 1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Value": "0 or 1"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[0 or 1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Value": "0 or 1"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[0 or 1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Value": "0 or 1"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[0 or 1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "35.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Value": "0 or 1"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[0 or 1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Value": "1 to 5"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[1 to 5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Value": "1 to 5"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[1 to 5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Automatic",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Value": "1 to 5"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[1 to 5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "41.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Value": "1 to 5"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[1 to 5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "4.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Value": "1 to 5"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[1 to 5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Value": "1 to 5"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[1 to 5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Value": "1 to 5"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[1 to 5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "27.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Value": "1 to 5"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[1 to 5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Value": "1 to 5"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[1 to 5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Value": "1 to 5"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[1 to 5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Value": "1 to 5"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[1 to 5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "35.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Value": "1 to 5"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[1 to 5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human",
                        "Number of participants": "3"
                    },
                    "measures": "[Human]",
                    "outcomes": "[3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human",
                        "Number of participants": "3"
                    },
                    "measures": "[Human]",
                    "outcomes": "[3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Automatic",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human",
                        "Number of participants": "3"
                    },
                    "measures": "[Human]",
                    "outcomes": "[3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "41.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human",
                        "Number of participants": "3"
                    },
                    "measures": "[Human]",
                    "outcomes": "[3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "4.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human",
                        "Number of participants": "3"
                    },
                    "measures": "[Human]",
                    "outcomes": "[3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human",
                        "Number of participants": "3"
                    },
                    "measures": "[Human]",
                    "outcomes": "[3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human",
                        "Number of participants": "3"
                    },
                    "measures": "[Human]",
                    "outcomes": "[3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "27.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human",
                        "Number of participants": "3"
                    },
                    "measures": "[Human]",
                    "outcomes": "[3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human",
                        "Number of participants": "3"
                    },
                    "measures": "[Human]",
                    "outcomes": "[3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human",
                        "Number of participants": "3"
                    },
                    "measures": "[Human]",
                    "outcomes": "[3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human",
                        "Number of participants": "3"
                    },
                    "measures": "[Human]",
                    "outcomes": "[3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "35.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human",
                        "Number of participants": "3"
                    },
                    "measures": "[Human]",
                    "outcomes": "[3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "LCR",
                        "Voting method": "Majority vote"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[Majority vote]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "LCR",
                        "Voting method": "Majority vote"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[Majority vote]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Automatic",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "LCR",
                        "Voting method": "Majority vote"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[Majority vote]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "41.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "LCR",
                        "Voting method": "Majority vote"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[Majority vote]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "4.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "LCR",
                        "Voting method": "Majority vote"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[Majority vote]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "LCR",
                        "Voting method": "Majority vote"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[Majority vote]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "LCR",
                        "Voting method": "Majority vote"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[Majority vote]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "27.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "LCR",
                        "Voting method": "Majority vote"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[Majority vote]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "LCR",
                        "Voting method": "Majority vote"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[Majority vote]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "LCR",
                        "Voting method": "Majority vote"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[Majority vote]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "LCR",
                        "Voting method": "Majority vote"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[Majority vote]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "35.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "LCR",
                        "Voting method": "Majority vote"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[Majority vote]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Grammar",
                        "Scoring method": "Average"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[Average]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Grammar",
                        "Scoring method": "Average"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[Average]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Automatic",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Grammar",
                        "Scoring method": "Average"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[Average]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "41.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Grammar",
                        "Scoring method": "Average"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[Average]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "4.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Grammar",
                        "Scoring method": "Average"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[Average]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Grammar",
                        "Scoring method": "Average"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[Average]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Grammar",
                        "Scoring method": "Average"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[Average]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "27.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Grammar",
                        "Scoring method": "Average"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[Average]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Grammar",
                        "Scoring method": "Average"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[Average]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Grammar",
                        "Scoring method": "Average"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[Average]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Grammar",
                        "Scoring method": "Average"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[Average]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "35.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Grammar",
                        "Scoring method": "Average"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[Average]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Description": "100 examples",
                        "Sampling method": "Random"
                    },
                    "measures": "[Description]",
                    "outcomes": "[100 examples, Random]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Description": "100 examples",
                        "Sampling method": "Random"
                    },
                    "measures": "[Description]",
                    "outcomes": "[100 examples, Random]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Automatic",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Description": "100 examples",
                        "Sampling method": "Random"
                    },
                    "measures": "[Description]",
                    "outcomes": "[100 examples, Random]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "41.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Description": "100 examples",
                        "Sampling method": "Random"
                    },
                    "measures": "[Description]",
                    "outcomes": "[100 examples, Random]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "4.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Description": "100 examples",
                        "Sampling method": "Random"
                    },
                    "measures": "[Description]",
                    "outcomes": "[100 examples, Random]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Description": "100 examples",
                        "Sampling method": "Random"
                    },
                    "measures": "[Description]",
                    "outcomes": "[100 examples, Random]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Description": "100 examples",
                        "Sampling method": "Random"
                    },
                    "measures": "[Description]",
                    "outcomes": "[100 examples, Random]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "27.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Description": "100 examples",
                        "Sampling method": "Random"
                    },
                    "measures": "[Description]",
                    "outcomes": "[100 examples, Random]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Description": "100 examples",
                        "Sampling method": "Random"
                    },
                    "measures": "[Description]",
                    "outcomes": "[100 examples, Random]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Description": "100 examples",
                        "Sampling method": "Random"
                    },
                    "measures": "[Description]",
                    "outcomes": "[100 examples, Random]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Description": "100 examples",
                        "Sampling method": "Random"
                    },
                    "measures": "[Description]",
                    "outcomes": "[100 examples, Random]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "35.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Description": "100 examples",
                        "Sampling method": "Random"
                    },
                    "measures": "[Description]",
                    "outcomes": "[100 examples, Random]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "6",
                        "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                    },
                    "measures": "[Table]",
                    "outcomes": "[6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "6",
                        "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                    },
                    "measures": "[Table]",
                    "outcomes": "[6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Automatic",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "6",
                        "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                    },
                    "measures": "[Table]",
                    "outcomes": "[6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "41.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "6",
                        "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                    },
                    "measures": "[Table]",
                    "outcomes": "[6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "4.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "6",
                        "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                    },
                    "measures": "[Table]",
                    "outcomes": "[6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "6",
                        "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                    },
                    "measures": "[Table]",
                    "outcomes": "[6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "6",
                        "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                    },
                    "measures": "[Table]",
                    "outcomes": "[6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "27.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "6",
                        "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                    },
                    "measures": "[Table]",
                    "outcomes": "[6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "6",
                        "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                    },
                    "measures": "[Table]",
                    "outcomes": "[6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "6",
                        "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                    },
                    "measures": "[Table]",
                    "outcomes": "[6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "6",
                        "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                    },
                    "measures": "[Table]",
                    "outcomes": "[6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "35.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "6",
                        "Caption": "BLEU scores on the development and test sets, and human evaluations of logic correctness rate (LCR) and grammar check on the 100 examples randomly sampled from the test set"
                    },
                    "measures": "[Table]",
                    "outcomes": "[6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.6"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 11,
        "total_ground_truth_claims": 12,
        "number_of_matches": 0
    },
    "2008.04759_1": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "86.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 81.6, 87.2",
                        "Test (lf": "ex), 80.7, 86.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova, BERT-Large-Uncased, 81.6, 87.2, 80.7, 86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "86.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 83.8, 89.5",
                        "Test (lf": "ex), 83.3, 88.7"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL, MT-DNN, 83.8, 89.5, 83.3, 88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "86.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 83.5, 88.9",
                        "Test (lf": "ex), 83.4, 88.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, BERT-Large-Uncased, 83.5, 88.9, 83.4, 88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "86.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 83.6, 89.1",
                        "Test (lf": "ex), 83.8, 89.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet, RoBERTa-Large, 83.6, 89.1, 83.8, 89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "86.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 84.2, 90.2",
                        "Test (lf": "ex), 83.6, 89.6"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[SQLova + EG, BERT-Large-Uncased, 84.2, 90.2, 83.6, 89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "86.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dev (lf": "ex), 86.2, 92.3",
                        "Test (lf": "ex), 86.0, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[X-SQL + EG, MT-DNN, 86.2, 92.3, 86.0, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "86.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dev (lf": "ex), 86.6, 92.2",
                        "Test (lf": "ex), 86.2, 91.8"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, BERT-Large-Uncased, 86.6, 92.2, 86.2, 91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "86.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dev (lf": "ex), 86.6, 92.4",
                        "Test (lf": "ex), 86.5, 92.2"
                    },
                    "measures": "[Model, Base Model, Dev lf, Dev ex, Test lf, Test ex]",
                    "outcomes": "[HydraNet + EG, RoBERTa-Large, 86.6, 92.4, 86.5, 92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 8,
        "total_ground_truth_claims": 16,
        "number_of_matches": 0
    },
    "2208.04415_3": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "ENG",
                        "Accuracy": "31.8%",
                        "Category": "Easy"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "31.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "C-ML",
                        "Accuracy": "27.3%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "11.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "C-ML",
                        "Accuracy": "27.3%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "C-ML",
                        "Accuracy": "27.3%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "C-ML",
                        "Accuracy": "27.3%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "C-ML",
                        "Accuracy": "27.3%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "27.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "C-S",
                        "Accuracy": "23.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "11.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "C-S",
                        "Accuracy": "23.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "C-S",
                        "Accuracy": "23.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "C-S",
                        "Accuracy": "23.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "C-S",
                        "Accuracy": "23.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "C-S",
                        "Accuracy": "23.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "C-S",
                        "Accuracy": "23.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "C-S",
                        "Accuracy": "23.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "12.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "C-S",
                        "Accuracy": "23.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "23.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-ML",
                        "Accuracy": "21.4%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "11.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-ML",
                        "Accuracy": "21.4%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-ML",
                        "Accuracy": "21.4%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-ML",
                        "Accuracy": "21.4%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-ML",
                        "Accuracy": "21.4%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-ML",
                        "Accuracy": "21.4%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-ML",
                        "Accuracy": "21.4%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-ML",
                        "Accuracy": "21.4%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "12.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-ML",
                        "Accuracy": "21.4%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-ML",
                        "Accuracy": "21.4%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-ML",
                        "Accuracy": "21.4%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-ML",
                        "Accuracy": "21.4%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-ML",
                        "Accuracy": "21.4%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "21.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "11.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "12.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "10.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "19.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "18.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "17.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WY-S",
                        "Accuracy": "20.2%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "11.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "12.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "10.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-ML",
                        "Accuracy": "19.8%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "19.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "11.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "12.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "10.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "HT",
                        "Category": "WJ-S",
                        "Accuracy": "20.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "11.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "12.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "10.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "C-ML",
                        "Accuracy": "18.1%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "18.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "11.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "12.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "10.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "TABLE III",
                        "Caption": "Experimental Result of Sequence to Tree Model",
                        "Dataset": "MT",
                        "Category": "WY-ML",
                        "Accuracy": "17.9%"
                    },
                    "measures": "[Table, TABLE III]",
                    "outcomes": "[Caption, Experimental Result of Sequence to Tree Model]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "17.9"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 9,
        "total_ground_truth_claims": 45,
        "number_of_matches": 8
    },
    "2310.13575_6": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "1",
                        "Support": "189"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "87.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "1",
                        "Support": "189"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "2",
                        "Support": "277"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "86.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "2",
                        "Support": "277"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "3",
                        "Support": "191"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "85.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "3",
                        "Support": "191"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "4",
                        "Support": "124"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "75.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "4",
                        "Support": "124"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "62.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "5",
                        "Support": "164"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "67.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "5",
                        "Support": "164"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "54.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "6",
                        "Support": "27"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "48.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "6",
                        "Support": "27"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "25.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "7",
                        "Support": "44"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "31.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "7",
                        "Support": "44"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "22.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "\u22658",
                        "Support": "18"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "11.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "\u22658",
                        "Support": "18"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "24.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "Overall",
                        "Support": "1034"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Execution Accuracy, 87.3%]",
                        "[Execution Accuracy, 86.6%]",
                        "[Execution Accuracy, 85.3%]",
                        "[Execution Accuracy, 75.0%]",
                        "[Execution Accuracy, 67.1%]",
                        "[Execution Accuracy, 48.2%]",
                        "[Execution Accuracy, 31.8%]",
                        "[Execution Accuracy, 11.9%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "Overall",
                        "Support": "1034"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "69.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "1",
                        "Support": "189"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "87.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "1",
                        "Support": "189"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "2",
                        "Support": "277"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "86.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "2",
                        "Support": "277"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "3",
                        "Support": "191"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "85.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "3",
                        "Support": "191"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "4",
                        "Support": "124"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "75.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "4",
                        "Support": "124"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "62.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "5",
                        "Support": "164"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "67.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "5",
                        "Support": "164"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "54.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "6",
                        "Support": "27"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "48.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "6",
                        "Support": "27"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "25.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "7",
                        "Support": "44"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "31.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "7",
                        "Support": "44"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "22.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "\u22658",
                        "Support": "18"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "11.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "\u22658",
                        "Support": "18"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "24.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "Overall",
                        "Support": "1034"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall"
                    },
                    "measures": "[QPL Length, QPL Length]",
                    "outcomes": [
                        "[Q+QD, 78.3%]",
                        "[Q+QD, 83.4%]",
                        "[Q+QD, 78.0%]",
                        "[Q+QD, 62.9%]",
                        "[Q+QD, 54.4%]",
                        "[Q+QD, 25.9%]",
                        "[Q+QD, 22.7%]",
                        "[Q+QD, 24.4%]",
                        "[Support, 189]",
                        "[Support, 277]",
                        "[Support, 191]",
                        "[Support, 124]",
                        "[Support, 164]",
                        "[Support, 27]",
                        "[Support, 44]",
                        "[Support, 18]",
                        "[Support, 1034]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "Overall",
                        "Support": "1034"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "69.1"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 2,
        "total_ground_truth_claims": 18,
        "number_of_matches": 0
    },
    "1807.03100_1": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "72.5",
                        "Metric": "Test Accuracy",
                        "Outcome": "72.5"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "72.5",
                        "Metric": "Test Accuracy",
                        "Outcome": "72.5"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "72.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "71.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "71.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "71.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "71.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "71.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "71.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "71.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "76.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "79.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "84.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "76.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "79.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "84.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "76.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "79.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "84.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "76.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "79.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "84.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "79.2",
                        "Metric": "Test Accuracy",
                        "Outcome": "79.2"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "79.2",
                        "Metric": "Test Accuracy",
                        "Outcome": "79.2"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "79.2",
                        "Metric": "Test Accuracy",
                        "Outcome": "79.2"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "79.2",
                        "Metric": "Test Accuracy",
                        "Outcome": "79.2"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "79.2",
                        "Metric": "Test Accuracy",
                        "Outcome": "79.2"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "79.2",
                        "Metric": "Test Accuracy",
                        "Outcome": "79.2"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "76.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "79.2",
                        "Metric": "Test Accuracy",
                        "Outcome": "79.2"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "79.2",
                        "Metric": "Test Accuracy",
                        "Outcome": "79.2"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "79.2",
                        "Metric": "Test Accuracy",
                        "Outcome": "79.2"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "79.2",
                        "Metric": "Test Accuracy",
                        "Outcome": "79.2"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "79.2",
                        "Metric": "Test Accuracy",
                        "Outcome": "79.2"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "79.2",
                        "Metric": "Test Accuracy",
                        "Outcome": "79.2"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "79.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "76.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "76.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "84.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "76.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "84.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "76.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "84.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "76.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "84.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "76.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "84.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "77.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "77.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Accuracy": "76.9",
                        "Metric": "Test Accuracy",
                        "Outcome": "76.9"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "76.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Accuracy": "78.3",
                        "Metric": "Test Accuracy",
                        "Outcome": "78.3"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "84.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.4",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.4"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Accuracy": "83.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "84.0",
                        "Metric": "Test Accuracy",
                        "Outcome": "84.0"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "84.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Accuracy": "83.8",
                        "Metric": "Test Accuracy",
                        "Outcome": "83.8"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "Test and Dev accuracy (%) of the models on WikiSQL data",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "Test and Dev accuracy (%) of the models on WikiSQL data",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "Test and Dev accuracy (%) of the models on WikiSQL data",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "Test and Dev accuracy (%) of the models on WikiSQL data",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "Test and Dev accuracy (%) of the models on WikiSQL data",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "Test and Dev accuracy (%) of the models on WikiSQL data",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "Test and Dev accuracy (%) of the models on WikiSQL data",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "Test and Dev accuracy (%) of the models on WikiSQL data",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "Test and Dev accuracy (%) of the models on WikiSQL data",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "Test and Dev accuracy (%) of the models on WikiSQL data",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "Test and Dev accuracy (%) of the models on WikiSQL data",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "Test and Dev accuracy (%) of the models on WikiSQL data",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "Test and Dev accuracy (%) of the models on WikiSQL data",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "Test and Dev accuracy (%) of the models on WikiSQL data",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "where Accsynsyn{}_{\\text{syn}} refers to syntactical accuracy and Accexex{}_{\\text{ex}} refers to execution accuracy",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "where Accsynsyn{}_{\\text{syn}} refers to syntactical accuracy and Accexex{}_{\\text{ex}} refers to execution accuracy",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "where Accsynsyn{}_{\\text{syn}} refers to syntactical accuracy and Accexex{}_{\\text{ex}} refers to execution accuracy",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "where Accsynsyn{}_{\\text{syn}} refers to syntactical accuracy and Accexex{}_{\\text{ex}} refers to execution accuracy",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "where Accsynsyn{}_{\\text{syn}} refers to syntactical accuracy and Accexex{}_{\\text{ex}} refers to execution accuracy",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "where Accsynsyn{}_{\\text{syn}} refers to syntactical accuracy and Accexex{}_{\\text{ex}} refers to execution accuracy",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "where Accsynsyn{}_{\\text{syn}} refers to syntactical accuracy and Accexex{}_{\\text{ex}} refers to execution accuracy",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "where Accsynsyn{}_{\\text{syn}} refers to syntactical accuracy and Accexex{}_{\\text{ex}} refers to execution accuracy",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "where Accsynsyn{}_{\\text{syn}} refers to syntactical accuracy and Accexex{}_{\\text{ex}} refers to execution accuracy",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "where Accsynsyn{}_{\\text{syn}} refers to syntactical accuracy and Accexex{}_{\\text{ex}} refers to execution accuracy",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "where Accsynsyn{}_{\\text{syn}} refers to syntactical accuracy and Accexex{}_{\\text{ex}} refers to execution accuracy",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "where Accsynsyn{}_{\\text{syn}} refers to syntactical accuracy and Accexex{}_{\\text{ex}} refers to execution accuracy",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "where Accsynsyn{}_{\\text{syn}} refers to syntactical accuracy and Accexex{}_{\\text{ex}} refers to execution accuracy",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "where Accsynsyn{}_{\\text{syn}} refers to syntactical accuracy and Accexex{}_{\\text{ex}} refers to execution accuracy",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "+ EG (kk) indicates that model outputs are generated using the execution-guided strategy with beam size kk",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "+ EG (kk) indicates that model outputs are generated using the execution-guided strategy with beam size kk",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "+ EG (kk) indicates that model outputs are generated using the execution-guided strategy with beam size kk",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "+ EG (kk) indicates that model outputs are generated using the execution-guided strategy with beam size kk",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "+ EG (kk) indicates that model outputs are generated using the execution-guided strategy with beam size kk",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "+ EG (kk) indicates that model outputs are generated using the execution-guided strategy with beam size kk",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "+ EG (kk) indicates that model outputs are generated using the execution-guided strategy with beam size kk",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "+ EG (kk) indicates that model outputs are generated using the execution-guided strategy with beam size kk",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "+ EG (kk) indicates that model outputs are generated using the execution-guided strategy with beam size kk",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "+ EG (kk) indicates that model outputs are generated using the execution-guided strategy with beam size kk",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "+ EG (kk) indicates that model outputs are generated using the execution-guided strategy with beam size kk",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "+ EG (kk) indicates that model outputs are generated using the execution-guided strategy with beam size kk",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "+ EG (kk) indicates that model outputs are generated using the execution-guided strategy with beam size kk",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Caption": "Table 1",
                        "Description": "+ EG (kk) indicates that model outputs are generated using the execution-guided strategy with beam size kk",
                        "Metric": "Caption",
                        "Outcome": "Table 1"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed",
                        "Metric": "Text",
                        "Outcome": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed",
                        "Metric": "Text",
                        "Outcome": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed",
                        "Metric": "Text",
                        "Outcome": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed",
                        "Metric": "Text",
                        "Outcome": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed",
                        "Metric": "Text",
                        "Outcome": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed",
                        "Metric": "Text",
                        "Outcome": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed",
                        "Metric": "Text",
                        "Outcome": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed",
                        "Metric": "Text",
                        "Outcome": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed",
                        "Metric": "Text",
                        "Outcome": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed",
                        "Metric": "Text",
                        "Outcome": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed",
                        "Metric": "Text",
                        "Outcome": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed",
                        "Metric": "Text",
                        "Outcome": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed",
                        "Metric": "Text",
                        "Outcome": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed",
                        "Metric": "Text",
                        "Outcome": "We report both the syntactical accuracy Accsynsyn{}_{\\text{syn}} corresponding to the ratio of predictions that are exactly the ground truth SQL query, as well as the execution accuracy Accexex{}_{\\text{ex}} corresponding to the ratio of predictions that return the same result as the ground truth when executed"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)",
                        "Metric": "Text",
                        "Outcome": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)",
                        "Metric": "Text",
                        "Outcome": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)",
                        "Metric": "Text",
                        "Outcome": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)",
                        "Metric": "Text",
                        "Outcome": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)",
                        "Metric": "Text",
                        "Outcome": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)",
                        "Metric": "Text",
                        "Outcome": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)",
                        "Metric": "Text",
                        "Outcome": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)",
                        "Metric": "Text",
                        "Outcome": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)",
                        "Metric": "Text",
                        "Outcome": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)",
                        "Metric": "Text",
                        "Outcome": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)",
                        "Metric": "Text",
                        "Outcome": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)",
                        "Metric": "Text",
                        "Outcome": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)",
                        "Metric": "Text",
                        "Outcome": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)",
                        "Metric": "Text",
                        "Outcome": "Note that the execution accuracy is higher than syntactical accuracy as syntactically different programs can generate the same results (e.g., programs differing only in predicate order)"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555",
                        "Metric": "Text",
                        "Outcome": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555",
                        "Metric": "Text",
                        "Outcome": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555",
                        "Metric": "Text",
                        "Outcome": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555",
                        "Metric": "Text",
                        "Outcome": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555",
                        "Metric": "Text",
                        "Outcome": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555",
                        "Metric": "Text",
                        "Outcome": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555",
                        "Metric": "Text",
                        "Outcome": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555",
                        "Metric": "Text",
                        "Outcome": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555",
                        "Metric": "Text",
                        "Outcome": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555",
                        "Metric": "Text",
                        "Outcome": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555",
                        "Metric": "Text",
                        "Outcome": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555",
                        "Metric": "Text",
                        "Outcome": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555",
                        "Metric": "Text",
                        "Outcome": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Text": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555",
                        "Metric": "Text",
                        "Outcome": "In execution-guided decoding, we report two model variants, one using a beam size of 333 and the other a beam size of 555"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 26,
        "total_ground_truth_claims": 24,
        "number_of_matches": 10
    },
    "2108.02866_9": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "18.69"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "50.28"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "68.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "68.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "68.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "68.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "68.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "68.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "68.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "68.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "68.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "68.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "74.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.89"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.89"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.63"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "87.13"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MAP",
                        "outcome": "13.15"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "MRR",
                        "outcome": "16.56"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "13.10"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "20.08"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "22.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "25.24"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "29.66"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "33.20"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "51.70"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "66.27"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "66.27"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "66.27"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "66.27"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "66.27"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "66.27"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "66.27"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "66.27"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "70.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "70.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "70.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "70.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "70.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "70.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "70.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "70.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "70.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "70.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "70.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "75.53"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "80.54"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "84.14"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.63"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "47.92"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "44.93"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.49"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.34"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "dataset": "Mix-SQuWiki",
                        "question_type": "SQuAD",
                        "reranker": "single reranker",
                        "metric": "recalls",
                        "outcome": "58.38"
                    },
                    "measures": "[dataset, metric, outcome]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 27,
        "total_ground_truth_claims": 40,
        "number_of_matches": 11
    },
    "2112.02212_3": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels",
                        "train": "7000",
                        "test": "1034",
                        "DT-Fixup": "75.0",
                        "+Ours": "77.2"
                    },
                    "measures": "[train, test, DT-Fixup, +Ours]",
                    "outcomes": "[1694, 248, 91.9, 92.7, 2777, 446, 80.9, 82.3, 1461, 174, 60.3, 65.5, 1068, 166, 48.8, 52.4, 7000, 1034, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels",
                        "train": "7000",
                        "test": "1034",
                        "DT-Fixup": "75.0",
                        "+Ours": "77.2"
                    },
                    "measures": "[train, test, DT-Fixup, +Ours]",
                    "outcomes": "[1694, 248, 91.9, 92.7, 2777, 446, 80.9, 82.3, 1461, 174, 60.3, 65.5, 1068, 166, 48.8, 52.4, 7000, 1034, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels",
                        "train": "7000",
                        "test": "1034",
                        "DT-Fixup": "75.0",
                        "+Ours": "77.2"
                    },
                    "measures": "[train, test, DT-Fixup, +Ours]",
                    "outcomes": "[1694, 248, 91.9, 92.7, 2777, 446, 80.9, 82.3, 1461, 174, 60.3, 65.5, 1068, 166, 48.8, 52.4, 7000, 1034, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels",
                        "train": "7000",
                        "test": "1034",
                        "DT-Fixup": "75.0",
                        "+Ours": "77.2"
                    },
                    "measures": "[train, test, DT-Fixup, +Ours]",
                    "outcomes": "[1694, 248, 91.9, 92.7, 2777, 446, 80.9, 82.3, 1461, 174, 60.3, 65.5, 1068, 166, 48.8, 52.4, 7000, 1034, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels",
                        "train": "7000",
                        "test": "1034",
                        "DT-Fixup": "75.0",
                        "+Ours": "77.2"
                    },
                    "measures": "[train, test, DT-Fixup, +Ours]",
                    "outcomes": "[1694, 248, 91.9, 92.7, 2777, 446, 80.9, 82.3, 1461, 174, 60.3, 65.5, 1068, 166, 48.8, 52.4, 7000, 1034, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "All",
                        "Train size": "7000",
                        "Test size": "1034"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "75.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "All",
                        "Train size": "7000",
                        "Test size": "1034"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "77.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels",
                        "train": "7000",
                        "test": "1034",
                        "+Ours": "77.2"
                    },
                    "measures": "[train, test, +Ours]",
                    "outcomes": "[7000, 1034, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels",
                        "train": "7000",
                        "test": "1034",
                        "+Ours": "77.2"
                    },
                    "measures": "[train, test, +Ours]",
                    "outcomes": "[7000, 1034, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels",
                        "train": "7000",
                        "test": "1034",
                        "+Ours": "77.2"
                    },
                    "measures": "[train, test, +Ours]",
                    "outcomes": "[7000, 1034, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels",
                        "train": "7000",
                        "test": "1034",
                        "+Ours": "77.2"
                    },
                    "measures": "[train, test, +Ours]",
                    "outcomes": "[7000, 1034, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels",
                        "train": "7000",
                        "test": "1034",
                        "+Ours": "77.2"
                    },
                    "measures": "[train, test, +Ours]",
                    "outcomes": "[7000, 1034, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels",
                        "train": "7000",
                        "test": "1034",
                        "+Ours": "77.2"
                    },
                    "measures": "[train, test, +Ours]",
                    "outcomes": "[7000, 1034, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels",
                        "train": "7000",
                        "test": "1034",
                        "+Ours": "77.2"
                    },
                    "measures": "[train, test, +Ours]",
                    "outcomes": "[7000, 1034, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels",
                        "train": "7000",
                        "test": "1034",
                        "+Ours": "77.2"
                    },
                    "measures": "[train, test, +Ours]",
                    "outcomes": "[7000, 1034, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels",
                        "train": "7000",
                        "test": "1034",
                        "+Ours": "77.2"
                    },
                    "measures": "[train, test, +Ours]",
                    "outcomes": "[7000, 1034, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "All",
                        "Train size": "7000",
                        "Test size": "1034"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "77.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value",
                        "hard categories": "categories",
                        "limited": "value",
                        "manual curation": "value",
                        "simple heuristics": "value"
                    },
                    "measures": "[hard, extra hard, hard categories, limited, manual curation, simple heuristics]",
                    "outcomes": "[66~{}6-points, 48.8, 52.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value",
                        "hard categories": "categories",
                        "limited": "value",
                        "manual curation": "value",
                        "simple heuristics": "value"
                    },
                    "measures": "[hard, extra hard, hard categories, limited, manual curation, simple heuristics]",
                    "outcomes": "[66~{}6-points, 48.8, 52.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value",
                        "hard categories": "categories",
                        "limited": "value",
                        "manual curation": "value",
                        "simple heuristics": "value"
                    },
                    "measures": "[hard, extra hard, hard categories, limited, manual curation, simple heuristics]",
                    "outcomes": "[66~{}6-points, 48.8, 52.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value",
                        "hard categories": "categories",
                        "limited": "value",
                        "manual curation": "value",
                        "simple heuristics": "value"
                    },
                    "measures": "[hard, extra hard, hard categories, limited, manual curation, simple heuristics]",
                    "outcomes": "[66~{}6-points, 48.8, 52.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value",
                        "hard categories": "categories",
                        "limited": "value",
                        "manual curation": "value",
                        "simple heuristics": "value"
                    },
                    "measures": "[hard, extra hard, hard categories, limited, manual curation, simple heuristics]",
                    "outcomes": "[66~{}6-points, 48.8, 52.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value",
                        "hard categories": "categories",
                        "limited": "value",
                        "manual curation": "value",
                        "simple heuristics": "value"
                    },
                    "measures": "[hard, extra hard, hard categories, limited, manual curation, simple heuristics]",
                    "outcomes": "[66~{}6-points, 48.8, 52.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value",
                        "hard categories": "categories",
                        "limited": "value",
                        "manual curation": "value",
                        "simple heuristics": "value"
                    },
                    "measures": "[hard, extra hard, hard categories, limited, manual curation, simple heuristics]",
                    "outcomes": "[66~{}6-points, 48.8, 52.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value",
                        "hard categories": "categories",
                        "limited": "value",
                        "manual curation": "value",
                        "simple heuristics": "value"
                    },
                    "measures": "[hard, extra hard, hard categories, limited, manual curation, simple heuristics]",
                    "outcomes": "[66~{}6-points, 48.8, 52.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels"
                    },
                    "measures": "[Yu et al., 2018, hardness levels, difficulty levels]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels"
                    },
                    "measures": "[Yu et al., 2018, hardness levels, difficulty levels]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels"
                    },
                    "measures": "[Yu et al., 2018, hardness levels, difficulty levels]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels"
                    },
                    "measures": "[Yu et al., 2018, hardness levels, difficulty levels]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels"
                    },
                    "measures": "[Yu et al., 2018, hardness levels, difficulty levels]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels"
                    },
                    "measures": "[Yu et al., 2018, hardness levels, difficulty levels]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels"
                    },
                    "measures": "[Yu et al., 2018, hardness levels, difficulty levels]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Yu et al.": "2018, value",
                        "hardness levels": "categories",
                        "difficulty levels": "levels"
                    },
                    "measures": "[Yu et al., 2018, hardness levels, difficulty levels]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value",
                        "difficulty levels": "value"
                    },
                    "measures": "[hardness levels, difficulty levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value",
                        "difficulty levels": "value"
                    },
                    "measures": "[hardness levels, difficulty levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value",
                        "difficulty levels": "value"
                    },
                    "measures": "[hardness levels, difficulty levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value",
                        "difficulty levels": "value"
                    },
                    "measures": "[hardness levels, difficulty levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value",
                        "difficulty levels": "value"
                    },
                    "measures": "[hardness levels, difficulty levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value",
                        "difficulty levels": "value"
                    },
                    "measures": "[hardness levels, difficulty levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value",
                        "difficulty levels": "value"
                    },
                    "measures": "[hardness levels, difficulty levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value",
                        "difficulty levels": "value"
                    },
                    "measures": "[hardness levels, difficulty levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "categories": "value"
                    },
                    "measures": "[categories]",
                    "outcomes": "[hard, extra hard]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hardness levels": "value"
                    },
                    "measures": "[hardness levels]",
                    "outcomes": "[categories]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "difficulty levels": "value"
                    },
                    "measures": "[difficulty levels]",
                    "outcomes": "[levels]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "train": "value",
                        "test": "value"
                    },
                    "measures": "[train, test]",
                    "outcomes": "[1694, 248, 2777, 446, 1461, 174, 1068, 166, 7000, 1034]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "DT-Fixup": "value",
                        "+Ours": "value"
                    },
                    "measures": "[DT-Fixup, +Ours]",
                    "outcomes": "[91.9, 92.7, 80.9, 82.3, 60.3, 65.5, 48.8, 52.4, 75.0, 77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Easy": "value",
                        "Medium": "value",
                        "Hard": "value",
                        "Extra": "value",
                        "All": "value"
                    },
                    "measures": "[Easy, Medium, Hard, Extra, All]",
                    "outcomes": "[91.9, 80.9, 60.3, 48.8, 75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "hard": "value",
                        "extra hard": "value"
                    },
                    "measures": "[hard, extra hard]",
                    "outcomes": "[66~{}6-points]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 67,
        "total_ground_truth_claims": 10,
        "number_of_matches": 2
    },
    "2310.18662_7": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "CoSQL"
                    },
                    "measures": "[name, value]",
                    "outcomes": "[metric, outcome]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "Spider",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "206.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "CoSQL"
                    },
                    "measures": "[name, value]",
                    "outcomes": "[metric, outcome]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "SParC",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "191.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "CoSQL"
                    },
                    "measures": "[name, value]",
                    "outcomes": "[metric, outcome]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "CoSQL"
                    },
                    "measures": "[name, value]",
                    "outcomes": "[metric, outcome]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "Spider",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "237.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "CoSQL"
                    },
                    "measures": "[name, value]",
                    "outcomes": "[metric, outcome]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "SParC",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "200.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "CoSQL"
                    },
                    "measures": "[name, value]",
                    "outcomes": "[metric, outcome]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "199.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "LSTM",
                        "value": "206.6"
                    },
                    "measures": "[name, Spider]",
                    "outcomes": [
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "Spider",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "206.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "Spider"
                    },
                    "measures": "[value, 191.5]",
                    "outcomes": [
                        "[name, SParC]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "SParC",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "191.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "Spider"
                    },
                    "measures": "[value, 191.5]",
                    "outcomes": [
                        "[name, SParC]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "Spider"
                    },
                    "measures": "[value, 191.5]",
                    "outcomes": [
                        "[name, SParC]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "Spider",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "237.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "Spider"
                    },
                    "measures": "[value, 191.5]",
                    "outcomes": [
                        "[name, SParC]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "SParC",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "200.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "Spider"
                    },
                    "measures": "[value, 191.5]",
                    "outcomes": [
                        "[name, SParC]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "199.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "SParC"
                    },
                    "measures": "[value, 200.7]",
                    "outcomes": [
                        "[name, CoSQL]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "SParC",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "191.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "SParC"
                    },
                    "measures": "[value, 200.7]",
                    "outcomes": [
                        "[name, CoSQL]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "SParC"
                    },
                    "measures": "[value, 200.7]",
                    "outcomes": [
                        "[name, CoSQL]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "Spider",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "237.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "SParC"
                    },
                    "measures": "[value, 200.7]",
                    "outcomes": [
                        "[name, CoSQL]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "SParC",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "200.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "CoSQL"
                    },
                    "measures": "[value, 201.0]",
                    "outcomes": [
                        "[name, LSTM]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "SParC",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "191.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "CoSQL"
                    },
                    "measures": "[value, 201.0]",
                    "outcomes": [
                        "[name, LSTM]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "ASTormer",
                        "value": "237.0"
                    },
                    "measures": "[name, Spider]",
                    "outcomes": [
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "SParC",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "191.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "ASTormer",
                        "value": "237.0"
                    },
                    "measures": "[name, Spider]",
                    "outcomes": [
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "Spider",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "237.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "Spider"
                    },
                    "measures": "[value, 200.7]",
                    "outcomes": [
                        "[name, SParC]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "SParC",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "191.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "Spider"
                    },
                    "measures": "[value, 200.7]",
                    "outcomes": [
                        "[name, SParC]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "199.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "SParC"
                    },
                    "measures": "[value, 199.1]",
                    "outcomes": [
                        "[name, CoSQL]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "SParC",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "191.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "SParC"
                    },
                    "measures": "[value, 199.1]",
                    "outcomes": [
                        "[name, CoSQL]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "199.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "CoSQL"
                    },
                    "measures": "[value, 199.1]",
                    "outcomes": [
                        "[name, ASTormer]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "SParC",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "191.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "name": "CoSQL"
                    },
                    "measures": "[value, 199.1]",
                    "outcomes": [
                        "[name, ASTormer]",
                        "[metric, inference time]",
                        "[outcome, seconds/per 100010001000 samples under the same configuration]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "199.1"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 9,
        "total_ground_truth_claims": 6,
        "number_of_matches": 5
    },
    "2311.01173_3": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.29/0.33"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.29/0.33]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.35/0.41"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.35/0.41]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.35/0.41"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.35/0.41]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.35"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.45/0.53"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.45/0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.45/0.53"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.45/0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.45/0.53"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.45/0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.48/0.57"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.57]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.48/0.57"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.57]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.48/0.57"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.57]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.48/0.57"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.57]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.48/0.59"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.48/0.59"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.48/0.59"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.48/0.59"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.48/0.59"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.48/0.59"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.48/0.59"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.48/0.59"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.48/0.59"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.48/0.59"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.48/0.59"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.48/0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Accuracy": "0.51/0.62"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.51/0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Accuracy": "0.51/0.62"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.51/0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Accuracy": "0.51/0.62"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.51/0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Accuracy": "0.51/0.62"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.51/0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Accuracy": "0.51/0.62"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.51/0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Accuracy": "0.51/0.62"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.51/0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Accuracy": "0.51/0.62"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.51/0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.62"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.35/0.39"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.35/0.39]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.35/0.39"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.35/0.39]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.35/0.39"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.35/0.39]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.35/0.39"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.35/0.39]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.35/0.39"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.35/0.39]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.35/0.39"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.35/0.39]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.35/0.39"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.35/0.39]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.39"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.46/0.53"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.46/0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.46/0.53"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.46/0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.46/0.53"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.46/0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.46/0.53"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.46/0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.46/0.53"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.46/0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.46/0.53"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.46/0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.46/0.53"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.46/0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.52/0.60"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.52/0.60"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.52/0.60"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.52/0.60"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.52/0.60"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.52/0.60"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.52/0.60"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.60"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.53/0.64"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.53/0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.53/0.64"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.53/0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.53/0.64"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.53/0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.53/0.64"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.53/0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.53/0.64"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.53/0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.53/0.64"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.53/0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.53/0.64"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.53/0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.54/0.64"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.54/0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.54/0.64"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.54/0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.54/0.64"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.54/0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.54/0.64"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.54/0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.54/0.64"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.54/0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.54/0.64"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.54/0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.54/0.64"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.54/0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.52/0.63"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.52/0.63"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.52/0.63"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.52/0.63"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.52/0.63"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.52/0.63"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.52/0.63"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.63"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Accuracy": "0.52/0.62"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Accuracy": "0.52/0.62"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Accuracy": "0.52/0.62"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Accuracy": "0.52/0.62"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Accuracy": "0.52/0.62"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Accuracy": "0.52/0.62"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Accuracy": "0.52/0.62"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.52/0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "100",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.62"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.03/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.03/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.03/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.03/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.03/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.03/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.03/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.03/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.03/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.03/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.03/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.03/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.03/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.03/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.07"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.05/0.10"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.05/0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.05/0.10"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.05/0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.05/0.10"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.05/0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.05/0.10"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.05/0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.05/0.10"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.05/0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.05/0.10"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.05/0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.05/0.10"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.05/0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.10"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.07/0.13"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.07/0.13"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.07/0.13"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.07/0.13"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.07/0.13"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.07/0.13"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.07/0.13"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.13"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.15"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.09/0.16"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.09/0.16"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.09/0.16"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.09/0.16"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.09/0.16"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.09/0.16"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.09/0.16"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.16"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.10/0.18"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.10/0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.10/0.18"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.10/0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.10/0.18"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.10/0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.10/0.18"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.10/0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.10/0.18"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.10/0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.10/0.18"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.10/0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.10/0.18"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.10/0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.18"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.04/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.04/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.04/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.04/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.04/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.04/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.04/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.04/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.04/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.04/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.04/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.04/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.04/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.04/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.04/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.04/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.11"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.04/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.04/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.04/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.04/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.04/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.04/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Accuracy": "0.04/0.07"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.04/0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.07/0.11"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.07/0.11"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.07/0.11"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.07/0.11"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.07/0.11"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.07/0.11"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.07/0.11"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Accuracy": "0.07/0.11"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.07/0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.11"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Accuracy": "0.09/0.15"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.09/0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.19"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Accuracy": "0.11/0.19"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.11/0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.12/0.21"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.12/0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.12/0.21"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.12/0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.12/0.21"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.12/0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.12/0.21"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.12/0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.12/0.21"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.12/0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.12/0.21"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.12/0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.12/0.21"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.12/0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.12/0.21"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.12/0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.12/0.21"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.12/0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Data set": "CRUSH (ours)",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Accuracy": "0.12/0.21"
                    },
                    "measures": "[Data set, Method, Budget]",
                    "outcomes": "[0.12/0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.21"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 26,
        "total_ground_truth_claims": 32,
        "number_of_matches": 23
    },
    "2109.10540_4": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.6"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "37.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.7"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "56.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "2.1"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "44.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.1"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "63.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "2.5"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "47.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.7"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.3"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "53.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.5"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "42.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.8"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "61.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "49.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.2"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "47.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.2"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "66.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dev": "46.6",
                        "Test": "0.5"
                    },
                    "measures": "[Model, ALIGN+BERT]",
                    "outcomes": [
                        "[Dev, 44.7]",
                        "[Test, 63.8]",
                        "[Dev, 2.1]",
                        "[Test, 1.1]",
                        "[Dev, 51.8]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.2"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "54.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.6"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "37.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.7"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "56.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "2.1"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "44.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.1"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "63.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "2.5"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "47.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.7"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.3"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "53.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.5"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "42.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.8"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "61.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "49.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.2"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "47.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.2"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "66.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dev": "53.8",
                        "Test": "0.3"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 42.2]",
                        "[Test, 61.3]",
                        "[Dev, 1.5]",
                        "[Test, 0.8]",
                        "[Dev, 49.7]",
                        "[Test, 0.4]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.2"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "54.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.6"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "37.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.7"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "56.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "2.1"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "44.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.1"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "63.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "2.5"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "47.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.7"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.3"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "53.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.5"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "42.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.8"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "61.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "49.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.2"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "47.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.2"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "66.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dev": "54.1",
                        "Test": "0.2"
                    },
                    "measures": "[Model, ALIGN]",
                    "outcomes": [
                        "[Dev, 37.8]",
                        "[Test, 56.9]",
                        "[Dev, 0.6]",
                        "[Test, 0.7]",
                        "[Dev, 46.6]",
                        "[Test, 0.5]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.2"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "54.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 7.17]",
                        "[outcome, 1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.6"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "37.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 7.17]",
                        "[outcome, 1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.7"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "56.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 7.17]",
                        "[outcome, 1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 7.17]",
                        "[outcome, 1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "2.1"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "44.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 7.17]",
                        "[outcome, 1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.1"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "63.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 7.17]",
                        "[outcome, 1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 7.17]",
                        "[outcome, 1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "2.5"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "47.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 7.17]",
                        "[outcome, 1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.7"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 7.17]",
                        "[outcome, 1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.3"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "53.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 7.17]",
                        "[outcome, 1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.5"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "42.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 7.17]",
                        "[outcome, 1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.8"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "61.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 7.17]",
                        "[outcome, 1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "49.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 7.17]",
                        "[outcome, 1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.2"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "47.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 7.17]",
                        "[outcome, 1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.2"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "66.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 9.89]",
                        "[outcome, 8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.6"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "37.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 9.89]",
                        "[outcome, 8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.7"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "56.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 9.89]",
                        "[outcome, 8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 9.89]",
                        "[outcome, 8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "2.1"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "44.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 9.89]",
                        "[outcome, 8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.1"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "63.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 9.89]",
                        "[outcome, 8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 9.89]",
                        "[outcome, 8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "2.5"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "47.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 9.89]",
                        "[outcome, 8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.7"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 9.89]",
                        "[outcome, 8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.3"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "53.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 9.89]",
                        "[outcome, 8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.5"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "42.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 9.89]",
                        "[outcome, 8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.8"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "61.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 9.89]",
                        "[outcome, 8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "49.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 9.89]",
                        "[outcome, 8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.2"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "47.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, 9.89]",
                        "[outcome, 8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.2"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "66.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, state-of-the-art]",
                        "[outcome, competitive]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.6"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "37.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, state-of-the-art]",
                        "[outcome, competitive]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.7"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "56.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, state-of-the-art]",
                        "[outcome, competitive]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, state-of-the-art]",
                        "[outcome, competitive]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "2.1"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "44.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, state-of-the-art]",
                        "[outcome, competitive]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.1"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "63.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, state-of-the-art]",
                        "[outcome, competitive]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, state-of-the-art]",
                        "[outcome, competitive]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "2.5"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "47.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, state-of-the-art]",
                        "[outcome, competitive]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.7"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, state-of-the-art]",
                        "[outcome, competitive]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.3"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "53.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, state-of-the-art]",
                        "[outcome, competitive]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.5"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "42.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, state-of-the-art]",
                        "[outcome, competitive]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.8"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "61.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, state-of-the-art]",
                        "[outcome, competitive]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "49.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, state-of-the-art]",
                        "[outcome, competitive]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.2"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "47.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Table": "4",
                        "WTQ": "test"
                    },
                    "measures": "[Table, 5]",
                    "outcomes": [
                        "[Spider]",
                        "[Ex.Set]",
                        "[metric, state-of-the-art]",
                        "[outcome, competitive]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.2"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "66.5"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 6,
        "total_ground_truth_claims": 15,
        "number_of_matches": 1
    }
}