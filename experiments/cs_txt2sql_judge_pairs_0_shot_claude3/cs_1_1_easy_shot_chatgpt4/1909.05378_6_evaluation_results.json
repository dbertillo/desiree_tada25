{
    "matches": [
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "evaluation_method": "human",
                    "evaluation_participants": "3 students proficient in English",
                    "evaluation_sample_size": "100 descriptions randomly sampled",
                    "evaluation_decision_method_LCR": "majority vote",
                    "evaluation_scale_grammar": "1 to 5 (larger is better)",
                    "BLEU_set": "development"
                },
                "measures": "[BLEU]",
                "outcomes": "[9.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.5"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "evaluation_method": "human",
                    "evaluation_participants": "3 students proficient in English",
                    "evaluation_sample_size": "100 descriptions randomly sampled",
                    "evaluation_decision_method_LCR": "majority vote",
                    "evaluation_scale_grammar": "1 to 5 (larger is better)",
                    "BLEU_set": "test"
                },
                "measures": "[BLEU]",
                "outcomes": "[9.3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "evaluation_method": "human",
                    "evaluation_participants": "3 students proficient in English",
                    "evaluation_sample_size": "100 descriptions randomly sampled",
                    "evaluation_decision_method_LCR": "majority vote",
                    "evaluation_scale_grammar": "1 to 5 (larger is better)"
                },
                "measures": "[LCR]",
                "outcomes": "[41.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Template",
                    "evaluation_method": "human",
                    "evaluation_participants": "3 students proficient in English",
                    "evaluation_sample_size": "100 descriptions randomly sampled",
                    "evaluation_decision_method_LCR": "majority vote",
                    "evaluation_scale_grammar": "1 to 5 (larger is better)"
                },
                "measures": "[grammar]",
                "outcomes": "[4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "evaluation_method": "human",
                    "evaluation_participants": "3 students proficient in English",
                    "evaluation_sample_size": "100 descriptions randomly sampled",
                    "evaluation_decision_method_LCR": "majority vote",
                    "evaluation_scale_grammar": "1 to 5 (larger is better)",
                    "BLEU_set": "development"
                },
                "measures": "[BLEU]",
                "outcomes": "[15.3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "evaluation_method": "human",
                    "evaluation_participants": "3 students proficient in English",
                    "evaluation_sample_size": "100 descriptions randomly sampled",
                    "evaluation_decision_method_LCR": "majority vote",
                    "evaluation_scale_grammar": "1 to 5 (larger is better)",
                    "BLEU_set": "test"
                },
                "measures": "[BLEU]",
                "outcomes": "[14.1]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "evaluation_method": "human",
                    "evaluation_participants": "3 students proficient in English",
                    "evaluation_sample_size": "100 descriptions randomly sampled",
                    "evaluation_decision_method_LCR": "majority vote",
                    "evaluation_scale_grammar": "1 to 5 (larger is better)"
                },
                "measures": "[LCR]",
                "outcomes": "[27.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Seq2Seq",
                    "evaluation_method": "human",
                    "evaluation_participants": "3 students proficient in English",
                    "evaluation_sample_size": "100 descriptions randomly sampled",
                    "evaluation_decision_method_LCR": "majority vote",
                    "evaluation_scale_grammar": "1 to 5 (larger is better)"
                },
                "measures": "[grammar]",
                "outcomes": "[3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "evaluation_method": "human",
                    "evaluation_participants": "3 students proficient in English",
                    "evaluation_sample_size": "100 descriptions randomly sampled",
                    "evaluation_decision_method_LCR": "majority vote",
                    "evaluation_scale_grammar": "1 to 5 (larger is better)",
                    "BLEU_set": "development"
                },
                "measures": "[BLEU]",
                "outcomes": "[16.4]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "evaluation_method": "human",
                    "evaluation_participants": "3 students proficient in English",
                    "evaluation_sample_size": "100 descriptions randomly sampled",
                    "evaluation_decision_method_LCR": "majority vote",
                    "evaluation_scale_grammar": "1 to 5 (larger is better)",
                    "BLEU_set": "test"
                },
                "measures": "[BLEU]",
                "outcomes": "[15.1]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "evaluation_method": "human",
                    "evaluation_participants": "3 students proficient in English",
                    "evaluation_sample_size": "100 descriptions randomly sampled",
                    "evaluation_decision_method_LCR": "majority vote",
                    "evaluation_scale_grammar": "1 to 5 (larger is better)"
                },
                "measures": "[LCR]",
                "outcomes": "[35.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "model": "Pointer-generator",
                    "evaluation_method": "human",
                    "evaluation_participants": "3 students proficient in English",
                    "evaluation_sample_size": "100 descriptions randomly sampled",
                    "evaluation_decision_method_LCR": "majority vote",
                    "evaluation_scale_grammar": "1 to 5 (larger is better)"
                },
                "measures": "[grammar]",
                "outcomes": "[3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "yes"
        }
    ],
    "total_extracted_claims": 12,
    "total_ground_truth_claims": 12,
    "number_of_matches": 12
}