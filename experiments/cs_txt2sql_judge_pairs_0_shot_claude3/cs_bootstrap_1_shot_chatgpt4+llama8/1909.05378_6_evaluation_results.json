{
    "matches": [
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Template",
                    "Metric": "BLEU",
                    "Dataset": "Development set",
                    "Evaluation": "Automatic"
                },
                "measures": "[BLEU]",
                "outcomes": "[9.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.5"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Model": "Template",
                    "Metric": "BLEU",
                    "Dataset": "Test set",
                    "Evaluation": "Automatic"
                },
                "measures": "[BLEU]",
                "outcomes": "[9.3]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Automatic",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "9.3"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Evaluation": "Papineni et al. (2002)"
                },
                "measures": "[BLEU score]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Evaluation": "Papineni et al. (2002)"
                },
                "measures": "[BLEU score]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Evaluation": "Papineni et al. (2002)"
                },
                "measures": "[BLEU score]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Evaluation": "Papineni et al. (2002)"
                },
                "measures": "[BLEU score]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Evaluation": "Papineni et al. (2002)"
                },
                "measures": "[BLEU score]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Evaluation": "Papineni et al. (2002)"
                },
                "measures": "[BLEU score]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Evaluation": "Papineni et al. (2002)"
                },
                "measures": "[BLEU score]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Evaluation": "Papineni et al. (2002)"
                },
                "measures": "[BLEU score]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Evaluation": "Papineni et al. (2002)"
                },
                "measures": "[BLEU score]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "BLEU score",
                    "Evaluation": "Papineni et al. (2002)"
                },
                "measures": "[BLEU score]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[LCR]",
                "outcomes": "[41.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "41.0"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[Grammar]",
                "outcomes": "[4.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Template",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "4.0"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[LCR]",
                "outcomes": "[27.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[LCR]",
                "outcomes": "[27.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[LCR]",
                "outcomes": "[27.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "27.0"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[Grammar]",
                "outcomes": "[3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[Grammar]",
                "outcomes": "[3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[Grammar]",
                "outcomes": "[3.5]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.5"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[LCR]",
                "outcomes": "[35.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[LCR]",
                "outcomes": "[35.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[LCR]",
                "outcomes": "[35.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[LCR]",
                "outcomes": "[35.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Logic correctness rate (LCR)",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[LCR]",
                "outcomes": "[35.0]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "LCR"
                ],
                "outcomes": [
                    "35.0"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[Grammar]",
                "outcomes": "[3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[Grammar]",
                "outcomes": "[3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[Grammar]",
                "outcomes": "[3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[Grammar]",
                "outcomes": "[3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Metric": "Grammar",
                    "Evaluation": "Human evaluation"
                },
                "measures": "[Grammar]",
                "outcomes": "[3.6]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Human",
                    "Sample Size": "100",
                    "Evaluators": "Three students proficient in English"
                },
                "measures": [
                    "Grammar"
                ],
                "outcomes": [
                    "3.6"
                ]
            },
            "match": "yes"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Method": "Randomly sampled",
                    "Description": "100 descriptions"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Method": "Randomly sampled",
                    "Description": "100 descriptions"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Method": "Randomly sampled",
                    "Description": "100 descriptions"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Method": "Randomly sampled",
                    "Description": "100 descriptions"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Method": "Majority vote",
                    "Description": "Final score"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Method": "Majority vote",
                    "Description": "Final score"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Method": "Majority vote",
                    "Description": "Final score"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Method": "Majority vote",
                    "Description": "Final score"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Method": "Average grammar score",
                    "Description": "Computed"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Method": "Average grammar score",
                    "Description": "Computed"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Method": "Average grammar score",
                    "Description": "Computed"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Method": "Average grammar score",
                    "Description": "Computed"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Three students proficient in English"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Three students proficient in English"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Three students proficient in English"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Three students proficient in English"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Score 0 or 1 for LCR"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Score 0 or 1 for LCR"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Score 0 or 1 for LCR"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Score 0 or 1 for LCR"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Score 1 to 5 for grammar check"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Score 1 to 5 for grammar check"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Score 1 to 5 for grammar check"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Score 1 to 5 for grammar check"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Larger, the better"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Larger, the better"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Larger, the better"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Larger, the better"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Final score decided by majority vote"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.3"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Final score decided by majority vote"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Seq2Seq",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "14.1"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Final score decided by majority vote"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Dev",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "16.4"
                ]
            },
            "match": "no"
        },
        {
            "Extracted_claim": {
                "subject": {
                    "Evaluation": "Human evaluation",
                    "Description": "Final score decided by majority vote"
                },
                "measures": "[]",
                "outcomes": "[]"
            },
            "Ground_truth_claim": {
                "subject": {
                    "Model": "Pointer-generator",
                    "Dataset": "Test",
                    "Evaluation": "Automatic"
                },
                "measures": [
                    "BLEU"
                ],
                "outcomes": [
                    "15.1"
                ]
            },
            "match": "no"
        }
    ],
    "total_extracted_claims": 17,
    "total_ground_truth_claims": 12,
    "number_of_matches": 8
}