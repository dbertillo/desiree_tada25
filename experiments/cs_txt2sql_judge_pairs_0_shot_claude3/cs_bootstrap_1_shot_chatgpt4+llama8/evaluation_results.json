{
    "2306.00739_4": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Few-shot SQL-PaLM",
                        "Dataset": "Spider Dev",
                        "Prompt design": "Concise",
                        "Adaptation setting": "0-shot"
                    },
                    "measures": "[EX Accuracy]",
                    "outcomes": "[81.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "EX"
                    ],
                    "outcomes": [
                        "81.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Few-shot SQL-PaLM",
                        "Dataset": "Spider Dev",
                        "Prompt design": "Concise",
                        "Adaptation setting": "0-shot"
                    },
                    "measures": "[TS Accuracy]",
                    "outcomes": "[76.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Few-shot SQL-PaLM",
                        "Dataset": "Spider Dev",
                        "Prompt design": "Verbose",
                        "Adaptation setting": "0-shot"
                    },
                    "measures": "[EX Accuracy]",
                    "outcomes": "[78.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "EX"
                    ],
                    "outcomes": [
                        "78.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Few-shot SQL-PaLM",
                        "Dataset": "Spider Dev",
                        "Prompt design": "Verbose",
                        "Adaptation setting": "0-shot"
                    },
                    "measures": "[TS Accuracy]",
                    "outcomes": "[70.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "0-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Few-shot SQL-PaLM",
                        "Dataset": "Spider Dev",
                        "Prompt design": "Concise",
                        "Adaptation setting": "4-shot"
                    },
                    "measures": "[EX Accuracy]",
                    "outcomes": "[82.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "EX"
                    ],
                    "outcomes": [
                        "82.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Few-shot SQL-PaLM",
                        "Dataset": "Spider Dev",
                        "Prompt design": "Concise",
                        "Adaptation setting": "4-shot"
                    },
                    "measures": "[TS Accuracy]",
                    "outcomes": "[77.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Concise",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Few-shot SQL-PaLM",
                        "Dataset": "Spider Dev",
                        "Prompt design": "Verbose",
                        "Adaptation setting": "4-shot"
                    },
                    "measures": "[EX Accuracy]",
                    "outcomes": "[81.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "EX"
                    ],
                    "outcomes": [
                        "81.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Few-shot SQL-PaLM",
                        "Dataset": "Spider Dev",
                        "Prompt design": "Verbose",
                        "Adaptation setting": "4-shot"
                    },
                    "measures": "[TS Accuracy]",
                    "outcomes": "[73.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Prompt design": "Verbose",
                        "Adaptation setting": "4-shot",
                        "Dataset": "Spider Dev",
                        "Method": "Few-shot SQL-PaLM",
                        "Model": "PaLM-2"
                    },
                    "measures": [
                        "TS"
                    ],
                    "outcomes": [
                        "73.7"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 8,
        "total_ground_truth_claims": 8,
        "number_of_matches": 8
    },
    "1909.00786_4": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Dev Set",
                        "Model": "SQLNet Xu et al. (2017)",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[10.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLNet",
                        "Dataset": "Spider",
                        "Citation": "Xu etal. (2017)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "10.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Test Set",
                        "Model": "SQLNet Xu et al. (2017)",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[12.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLNet",
                        "Dataset": "Spider",
                        "Citation": "Xu etal. (2017)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "12.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Dev Set",
                        "Model": "SyntaxSQLNet Yu et al. (2018b)",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[18.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "18.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Test Set",
                        "Model": "SyntaxSQLNet Yu et al. (2018b)",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[19.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "19.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Dev Set",
                        "Model": "+data augmentation Yu et al. (2018b)",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[24.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet + data augmentation",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Test Set",
                        "Model": "+data augmentation Yu et al. (2018b)",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[27.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SyntaxSQLNet + data augmentation",
                        "Dataset": "Spider",
                        "Citation": "Yu etal. (2018b)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "27.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Dev Set",
                        "Model": "Lee (2019)",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[28.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Lee (2019)",
                        "Dataset": "Spider",
                        "Citation": "Lee (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "28.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Test Set",
                        "Model": "Lee (2019)",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[24.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Lee (2019)",
                        "Dataset": "Spider",
                        "Citation": "Lee (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "24.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Dev Set",
                        "Model": "GNN Bogin et al. (2019)",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[40.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "GNN",
                        "Dataset": "Spider",
                        "Citation": "Bogin etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "40.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Test Set",
                        "Model": "GNN Bogin et al. (2019)",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[39.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "GNN",
                        "Dataset": "Spider",
                        "Citation": "Bogin etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "39.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Dev Set",
                        "Model": "IRNet Guo et al. (2019)",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[53.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "53.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Test Set",
                        "Model": "IRNet Guo et al. (2019)",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[46.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "46.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Dev Set",
                        "Model": "IRNet (BERT) Guo et al. (2019)",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[61.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet (BERT)",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "61.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Test Set",
                        "Model": "IRNet (BERT) Guo et al. (2019)",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[54.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet (BERT)",
                        "Dataset": "Spider",
                        "Citation": "Guo etal. (2019)",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "54.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Dev Set",
                        "Model": "Ours",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[36.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours",
                        "Dataset": "Spider",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "36.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Test Set",
                        "Model": "Ours",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[32.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours",
                        "Dataset": "Spider",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "32.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Dev Set",
                        "Model": "+ utterance-table BERT Embedding",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[57.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours + utterance-table BERT Embedding",
                        "Dataset": "Spider",
                        "Evaluation": "Dev Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "57.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Evaluation Set": "Test Set",
                        "Model": "+ utterance-table BERT Embedding",
                        "Context": "context-independent cross-domain text-to-SQL generation",
                        "Method": "standalone question handling without interaction-level decoder or query editing"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Ours + utterance-table BERT Embedding",
                        "Dataset": "Spider",
                        "Evaluation": "Test Set"
                    },
                    "measures": [
                        "Exact Set Match Accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 20,
        "total_ground_truth_claims": 18,
        "number_of_matches": 18
    },
    "2104.04689_2": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCNKelkar etal. (2020)",
                        "Hardness Level": "Easy",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[70.4%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCNKelkar etal. (2020)",
                        "Hardness Level": "Medium",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[54.1%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "54.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCNKelkar etal. (2020)",
                        "Hardness Level": "Hard",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[35.6%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "35.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCNKelkar etal. (2020)",
                        "Hardness Level": "Extra Hard",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[28.2%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "28.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCNKelkar etal. (2020)",
                        "Hardness Level": "All",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[50.7%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Citation": "Kelkar etal. (2020)",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "50.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Easy",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[78.9%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "78.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Medium",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[63.2%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Hard",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[46.6%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "Extra Hard",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[29.8%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "29.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN",
                        "Hardness Level": "All",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[58.7%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "58.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN+RAT",
                        "Hardness Level": "Easy",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[85.0%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "85.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN+RAT",
                        "Hardness Level": "Medium",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[70.9%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN+RAT",
                        "Hardness Level": "Hard",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[56.3%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "56.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN+RAT",
                        "Hardness Level": "Extra Hard",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[32.7%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "32.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "R-GCN+RAT",
                        "Hardness Level": "All",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[65.6%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "R-GCN+RAT",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "65.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "GPNN",
                        "Hardness Level": "Easy",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[87.5%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "GPNN",
                        "Hardness Level": "Medium",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[74.9%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "74.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "GPNN",
                        "Hardness Level": "Hard",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[59.2%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "59.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "GPNN",
                        "Hardness Level": "Extra Hard",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[41.6%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "41.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "GPNN",
                        "Hardness Level": "All",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[69.9%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "GPNN",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "69.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "RATSQL",
                        "Hardness Level": "Easy",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[87.1%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "RATSQL",
                        "Hardness Level": "Medium",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[74.9%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "74.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "RATSQL",
                        "Hardness Level": "Hard",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[57.5%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "57.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "RATSQL",
                        "Hardness Level": "Extra Hard",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[46.4%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "46.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "RATSQL",
                        "Hardness Level": "All",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[70.2%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "RATSQL",
                        "Implementation": "Custom",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "ShadowGNN",
                        "Hardness Level": "Easy",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[87.5%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Easy"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "87.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "ShadowGNN",
                        "Hardness Level": "Medium",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[78.0%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Medium"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "78.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "ShadowGNN",
                        "Hardness Level": "Hard",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[61.5%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "61.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "ShadowGNN",
                        "Hardness Level": "Extra Hard",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[45.8%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "Extra Hard"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "45.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Approach": "ShadowGNN",
                        "Hardness Level": "All",
                        "Dataset": "Development set",
                        "Source": "Yu etal. (2018)"
                    },
                    "measures": "[Match Accuracy]",
                    "outcomes": "[72.3%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Approaches": "ShadowGNN",
                        "Dataset": "development set",
                        "Hardness Level": "All"
                    },
                    "measures": [
                        "Match Accuracy"
                    ],
                    "outcomes": [
                        "72.3"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 30,
        "total_ground_truth_claims": 30,
        "number_of_matches": 30
    },
    "2212.09278_5": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[68.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "68.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[40.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "40.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[70.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "26.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[70.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "12.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[70.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "70.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[45.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "26.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[45.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "12.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[45.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "45.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[75.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "26.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[75.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "12.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[75.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "29.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[75.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "18.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[75.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "75.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[51.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "26.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[51.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "12.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[51.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "29.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[51.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "18.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[51.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "51.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "26.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "12.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "29.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "18.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "35.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "21.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "80.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[68.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "26.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[68.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "12.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[68.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "29.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[68.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "18.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[68.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "35.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[68.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "21.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[68.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "68.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[81.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "26.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[81.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "12.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[81.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "29.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[81.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "18.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[81.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "35.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[81.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "21.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[81.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "46.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[81.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "43.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Easy",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[81.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "81.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[66.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "26.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[66.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "12.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[66.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "29.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[66.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "18.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[66.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "35.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[66.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "21.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[66.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "46.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[66.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "43.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Medium",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[66.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[26.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "26.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[29.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "12.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[29.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "29.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[35.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "12.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[35.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "18.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[35.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "35.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[46.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "12.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[46.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "18.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[46.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "21.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[46.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "46.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[44.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "12.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[44.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "18.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[44.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "21.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[44.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "43.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Hard",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[44.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "44.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Extra",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[12.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "EditSQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "12.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Extra",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[18.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "IG-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "18.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "R<sup>2</sup>SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Extra",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[21.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "R2SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "21.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Extra",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[43.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "CQR-SQL",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "43.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "SQL Difficulty": "Extra",
                        "Task": "SQL prediction",
                        "Metric": "QM accuracy"
                    },
                    "measures": "[QM accuracy]",
                    "outcomes": "[41.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "MIGA+PICARD",
                        "Dataset": "SparC dev set",
                        "Task": "SQL prediction",
                        "Difficulty": "Extra"
                    },
                    "measures": [
                        "QM accuracy"
                    ],
                    "outcomes": [
                        "41.8"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 26,
        "total_ground_truth_claims": 20,
        "number_of_matches": 20
    },
    "2305.16253_7": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "42.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "42.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.72]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL (BERT)",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[42.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "42.21"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[70.00]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.73]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[11.55]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[11.55]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[11.55]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "UNISAR (BART)",
                        "Pre-trained Language Model": "BART",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[11.55]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.55"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[71.90]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[39.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[9.52]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[9.52]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[9.52]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[9.52]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[9.52]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[9.52]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "PICARD (T5)",
                        "Pre-trained Language Model": "T5",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[9.52]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.52"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v1subscript1v1",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "39.96"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[39.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.88]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[40.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "40.29"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[40.99]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[44.82]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "44.82"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[54.40]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[54.40]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[54.40]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[54.40]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[54.40]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[54.40]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "BiaSpider v2subscript2v2",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[54.40]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "54.40"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[52.96]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[43.69]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Neg",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[51.25]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[44.51]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Random-Pos",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[50.29]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Ori-ACC\u2191"
                    },
                    "measures": "[Ori-ACC]",
                    "outcomes": "[65.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "ACC\u2191"
                    },
                    "measures": "[ACC]",
                    "outcomes": "[41.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.85"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.37"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "10.02"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v1",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.96"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "55.79"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "52.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "11.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.43"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "UNISAR",
                        "Pre-trained Language Model": "BART",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "12.65"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.74"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.68"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.97"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "PICARD",
                        "Pre-trained Language Model": "T5",
                        "Dataset": "BiaSpider v2",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "9.58"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "RoBERTa-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "53.56"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Neg",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "51.25"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Random-Pos",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "50.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Comparative",
                        "Pre-trained Language Model": "BERT",
                        "Evaluation Context": "Stereotypical correlations with different judgemental modifiers",
                        "Dataset": "BiaSpider v1subscript1v1",
                        "Metric": "Bias Score\u2193"
                    },
                    "measures": "[Bias Score]",
                    "outcomes": "[49.71]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pre-trained Language Model": "BERT",
                        "Dataset": "BiaSpider v3",
                        "Bias Type": "Comparative",
                        "Evaluation Type": "Text-to-SQL",
                        "Number of Models Evaluated": "3"
                    },
                    "measures": [
                        "Bias Score"
                    ],
                    "outcomes": [
                        "49.71"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 36,
        "total_ground_truth_claims": 28,
        "number_of_matches": 8
    },
    "2211.06193_1": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Dataset": "Spider dev",
                        "Decoding": "constrained",
                        "Failure Category": "Incomplete SQL"
                    },
                    "measures": "[Percentage]",
                    "outcomes": "[6.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "Incomplete SQL"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "6.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Dataset": "Spider dev",
                        "Decoding": "constrained",
                        "Failure Category": "False Negatives"
                    },
                    "measures": "[Percentage]",
                    "outcomes": "[22.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "False Negatives"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "22.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Dataset": "Spider dev",
                        "Decoding": "constrained",
                        "Failure Category": "Foreign Keys"
                    },
                    "measures": "[Percentage]",
                    "outcomes": "[19.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "Foreign Keys"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "19.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Dataset": "Spider dev",
                        "Decoding": "constrained",
                        "Failure Category": "Logical Errors"
                    },
                    "measures": "[Percentage]",
                    "outcomes": "[2.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "Logical Errors"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "2.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Dataset": "Spider dev",
                        "Decoding": "constrained",
                        "Failure Category": "DK - Incorrect AGG"
                    },
                    "measures": "[Percentage]",
                    "outcomes": "[17.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "DK - Incorrect AGG"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "17.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Dataset": "Spider dev",
                        "Decoding": "constrained",
                        "Failure Category": "DK - Incorrect Table"
                    },
                    "measures": "[Percentage]",
                    "outcomes": "[3.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "DK - Incorrect Table"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "3.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Dataset": "Spider dev",
                        "Decoding": "constrained",
                        "Failure Category": "DK - Incorrect Column"
                    },
                    "measures": "[Percentage]",
                    "outcomes": "[13.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "DK - Incorrect Column"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "13.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Dataset": "Spider dev",
                        "Decoding": "constrained",
                        "Failure Category": "DK - Incorrect Value"
                    },
                    "measures": "[Percentage]",
                    "outcomes": "[3.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "DK - Incorrect Value"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "3.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Dataset": "Spider dev",
                        "Decoding": "constrained",
                        "Failure Category": "DK - Complex"
                    },
                    "measures": "[Percentage]",
                    "outcomes": "[11.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "T5+3B",
                        "Decoding Method": "constrained decoding",
                        "Dataset": "Spider dev",
                        "Error Analysis Category": "DK - Complex"
                    },
                    "measures": [
                        "Percentage"
                    ],
                    "outcomes": [
                        "11.0"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 26,
        "total_ground_truth_claims": 9,
        "number_of_matches": 9
    },
    "2205.02054_4": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider-SS",
                        "Comparison": "Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<= 1",
                        "Evaluation Set": "CG-SUBT"
                    },
                    "measures": "[Similarity]",
                    "outcomes": "[93.2%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "93.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider-SS",
                        "Comparison": "Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<= 2",
                        "Evaluation Set": "CG-SUBT"
                    },
                    "measures": "[Similarity]",
                    "outcomes": "[94.4%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider-SS",
                        "Comparison": "Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<= 1",
                        "Evaluation Set": "CG-SUBD"
                    },
                    "measures": "[Similarity]",
                    "outcomes": "[92.9%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider-SS",
                        "Comparison": "Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<= 2",
                        "Evaluation Set": "CG-SUBD"
                    },
                    "measures": "[Similarity]",
                    "outcomes": "[94.1%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-SUB_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "94.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider-SS",
                        "Comparison": "Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<= 1",
                        "Evaluation Set": "CG-APPT"
                    },
                    "measures": "[Similarity]",
                    "outcomes": "[86.0%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "86.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider-SS",
                        "Comparison": "Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<= 2",
                        "Evaluation Set": "CG-APPT"
                    },
                    "measures": "[Similarity]",
                    "outcomes": "[90.4%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_T",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "90.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider-SS",
                        "Comparison": "Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<= 1",
                        "Evaluation Set": "CG-APPD"
                    },
                    "measures": "[Similarity]",
                    "outcomes": "[88.9%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=1",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "Spider-SS",
                        "Comparison": "Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<= 2",
                        "Evaluation Set": "CG-APPD"
                    },
                    "measures": "[Similarity]",
                    "outcomes": "[92.6%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "CG-APP_D",
                        "Comparison": "Spider-SS and Spider-CG",
                        "Split Algorithm": "Same",
                        "Deviation": "<=2",
                        "Sentence Type": "Sub-sentences"
                    },
                    "measures": [
                        "Similarity"
                    ],
                    "outcomes": [
                        "92.6"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 13,
        "total_ground_truth_claims": 8,
        "number_of_matches": 8
    },
    "1909.05378_6": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Template",
                        "Metric": "BLEU",
                        "Dataset": "Development set",
                        "Evaluation": "Automatic"
                    },
                    "measures": "[BLEU]",
                    "outcomes": "[9.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Template",
                        "Metric": "BLEU",
                        "Dataset": "Test set",
                        "Evaluation": "Automatic"
                    },
                    "measures": "[BLEU]",
                    "outcomes": "[9.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Automatic",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "9.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Evaluation": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU score]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "41.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Evaluation": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU score]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "4.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Evaluation": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU score]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Evaluation": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU score]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Evaluation": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU score]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "27.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Evaluation": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU score]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Evaluation": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU score]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Evaluation": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU score]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Evaluation": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU score]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "35.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "BLEU score",
                        "Evaluation": "Papineni et al. (2002)"
                    },
                    "measures": "[BLEU score]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[41.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "41.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[4.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Template",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "4.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[27.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[27.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[27.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "27.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[3.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[35.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[35.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[35.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[35.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logic correctness rate (LCR)",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[LCR]",
                    "outcomes": "[35.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "LCR"
                    ],
                    "outcomes": [
                        "35.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Grammar",
                        "Evaluation": "Human evaluation"
                    },
                    "measures": "[Grammar]",
                    "outcomes": "[3.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Human",
                        "Sample Size": "100",
                        "Evaluators": "Three students proficient in English"
                    },
                    "measures": [
                        "Grammar"
                    ],
                    "outcomes": [
                        "3.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Randomly sampled",
                        "Description": "100 descriptions"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Randomly sampled",
                        "Description": "100 descriptions"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Randomly sampled",
                        "Description": "100 descriptions"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Randomly sampled",
                        "Description": "100 descriptions"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Majority vote",
                        "Description": "Final score"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Majority vote",
                        "Description": "Final score"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Majority vote",
                        "Description": "Final score"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Majority vote",
                        "Description": "Final score"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Average grammar score",
                        "Description": "Computed"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Average grammar score",
                        "Description": "Computed"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Average grammar score",
                        "Description": "Computed"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Average grammar score",
                        "Description": "Computed"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Three students proficient in English"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Three students proficient in English"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Three students proficient in English"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Three students proficient in English"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Score 0 or 1 for LCR"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Score 0 or 1 for LCR"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Score 0 or 1 for LCR"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Score 0 or 1 for LCR"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Score 1 to 5 for grammar check"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Score 1 to 5 for grammar check"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Score 1 to 5 for grammar check"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Score 1 to 5 for grammar check"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Larger, the better"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Larger, the better"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Larger, the better"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Larger, the better"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Final score decided by majority vote"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Final score decided by majority vote"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Seq2Seq",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Final score decided by majority vote"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Dev",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "16.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Evaluation": "Human evaluation",
                        "Description": "Final score decided by majority vote"
                    },
                    "measures": "[]",
                    "outcomes": "[]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-generator",
                        "Dataset": "Test",
                        "Evaluation": "Automatic"
                    },
                    "measures": [
                        "BLEU"
                    ],
                    "outcomes": [
                        "15.1"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 17,
        "total_ground_truth_claims": 12,
        "number_of_matches": 8
    },
    "2008.04759_1": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "86.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[80.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "86.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[88.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[88.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[89.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[83.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[89.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[91.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "91.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Logical Form Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Logical Form Accuracy]",
                    "outcomes": "[86.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Metric Type": "Execution Accuracy",
                        "Set": "Test"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[92.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Test"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logical Form Accuracy",
                        "Description": "percentage of exact matches of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logical Form Accuracy",
                        "Description": "percentage of exact matches of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logical Form Accuracy",
                        "Description": "percentage of exact matches of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logical Form Accuracy",
                        "Description": "percentage of exact matches of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logical Form Accuracy",
                        "Description": "percentage of exact matches of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logical Form Accuracy",
                        "Description": "percentage of exact matches of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logical Form Accuracy",
                        "Description": "percentage of exact matches of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Logical Form Accuracy",
                        "Description": "percentage of exact matches of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Execution Accuracy",
                        "Description": "percentage of exact matches of executed results of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Execution Accuracy",
                        "Description": "percentage of exact matches of executed results of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Execution Accuracy",
                        "Description": "percentage of exact matches of executed results of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Execution Accuracy",
                        "Description": "percentage of exact matches of executed results of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Execution Accuracy",
                        "Description": "percentage of exact matches of executed results of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Execution Accuracy",
                        "Description": "percentage of exact matches of executed results of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Execution Accuracy",
                        "Description": "percentage of exact matches of executed results of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Execution Accuracy",
                        "Description": "percentage of exact matches of executed results of predicted SQL queries and labels"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "consistently better than the other approaches"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "consistently better than the other approaches"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "consistently better than the other approaches"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "consistently better than the other approaches"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "consistently better than the other approaches"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "consistently better than the other approaches"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "consistently better than the other approaches"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "consistently better than the other approaches"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "significant better than SQLova, which uses the same base model, and is even as good as X-SQL, which uses MT-DNN as base model"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "significant better than SQLova, which uses the same base model, and is even as good as X-SQL, which uses MT-DNN as base model"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "significant better than SQLova, which uses the same base model, and is even as good as X-SQL, which uses MT-DNN as base model"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "significant better than SQLova, which uses the same base model, and is even as good as X-SQL, which uses MT-DNN as base model"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "significant better than SQLova, which uses the same base model, and is even as good as X-SQL, which uses MT-DNN as base model"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "significant better than SQLova, which uses the same base model, and is even as good as X-SQL, which uses MT-DNN as base model"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "significant better than SQLova, which uses the same base model, and is even as good as X-SQL, which uses MT-DNN as base model"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Description": "significant better than SQLova, which uses the same base model, and is even as good as X-SQL, which uses MT-DNN as base model"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "MT-DNN",
                        "Description": "significantly better than BERT-Large-Uncased (Liu et al., 2019a), and has similar score as RoBERTa on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "MT-DNN",
                        "Description": "significantly better than BERT-Large-Uncased (Liu et al., 2019a), and has similar score as RoBERTa on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "MT-DNN",
                        "Description": "significantly better than BERT-Large-Uncased (Liu et al., 2019a), and has similar score as RoBERTa on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "MT-DNN",
                        "Description": "significantly better than BERT-Large-Uncased (Liu et al., 2019a), and has similar score as RoBERTa on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "MT-DNN",
                        "Description": "significantly better than BERT-Large-Uncased (Liu et al., 2019a), and has similar score as RoBERTa on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "MT-DNN",
                        "Description": "significantly better than BERT-Large-Uncased (Liu et al., 2019a), and has similar score as RoBERTa on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "MT-DNN",
                        "Description": "significantly better than BERT-Large-Uncased (Liu et al., 2019a), and has similar score as RoBERTa on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "MT-DNN",
                        "Description": "significantly better than BERT-Large-Uncased (Liu et al., 2019a), and has similar score as RoBERTa on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa",
                        "Description": "similar score as MT-DNN on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa",
                        "Description": "similar score as MT-DNN on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa",
                        "Description": "similar score as MT-DNN on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa",
                        "Description": "similar score as MT-DNN on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "89.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa",
                        "Description": "similar score as MT-DNN on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "SQLova + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "90.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa",
                        "Description": "similar score as MT-DNN on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "X-SQL + EG",
                        "Base Model": "MT-DNN",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa",
                        "Description": "similar score as MT-DNN on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "BERT-Large-Uncased",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RoBERTa",
                        "Description": "similar score as MT-DNN on GLUE Benchmark"
                    }
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HydraNet + EG",
                        "Base Model": "RoBERTa-Large",
                        "Dataset": "WikiSQL",
                        "Split": "Dev"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "92.4"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 22,
        "total_ground_truth_claims": 16,
        "number_of_matches": 8
    },
    "2208.04415_3": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "ENG",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[31.8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "31.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "ENG",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[11.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "11.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "ENG",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "ENG",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.7%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "ENG",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[14.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ENG",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "14.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "27.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "12.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "23.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "21.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "10.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "19.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "18.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "17.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[27.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "23.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "21.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "10.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "19.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "18.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "17.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "23.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "21.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "10.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "19.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "18.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "17.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "23.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "21.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "10.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "19.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "18.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "17.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "23.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "21.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "10.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "19.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "18.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "17.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[12.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "C-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[23.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "23.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "C-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.7%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "C-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[6.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "C-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[1.7%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "C-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT C-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "WY-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[21.4%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "21.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "WY-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[8.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "WY-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[8.0%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "WY-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[1.7%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "WY-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[10.0%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "10.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "WY-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[20.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "WY-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[6.4%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "WY-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[6.7%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "6.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "WY-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[2.0%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "2.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "WY-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[8.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WY-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "WJ-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[19.8%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "19.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "WJ-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[8.6%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "WJ-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[5.0%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "WJ-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[1.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "WJ-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[9.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "9.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "HT",
                        "Model": "WJ-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[20.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "20.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "HT",
                        "Model": "WJ-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[5.0%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "HT",
                        "Model": "WJ-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[5.7%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "HT",
                        "Model": "WJ-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[1.7%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "1.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "HT",
                        "Model": "WJ-S",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[8.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "HT WJ-S",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "8.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "MT",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[18.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "18.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "MT",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[4.6%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "MT",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[5.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "5.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "MT",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[0.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "MT",
                        "Model": "Sequence to Tree Model",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT C-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[18.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "17.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[18.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[18.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[18.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[18.1%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[4.6%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "17.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[4.6%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[4.6%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[4.6%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[4.6%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[5.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "17.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[5.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[5.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[5.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[5.2%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[0.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "17.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[0.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[0.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[0.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[0.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "17.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "MT",
                        "Model": "C-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Easy",
                        "Dataset": "MT",
                        "Model": "WY-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[17.9%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Easy",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "17.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Medium",
                        "Dataset": "MT",
                        "Model": "WY-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[4.7%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Medium",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Hard",
                        "Dataset": "MT",
                        "Model": "WY-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[4.5%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "4.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "Extra Hard",
                        "Dataset": "MT",
                        "Model": "WY-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[0.3%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "Extra Hard",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "0.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Category": "All",
                        "Dataset": "MT",
                        "Model": "WY-ML",
                        "Source": "[41]"
                    },
                    "measures": "[41]",
                    "outcomes": [
                        "[Accuracy]",
                        "[7.6%]"
                    ]
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "MT WY-ML",
                        "Dataset": "All",
                        "Task": "Sequence to Tree",
                        "Study": "[41]"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "7.6"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 55,
        "total_ground_truth_claims": 45,
        "number_of_matches": 45
    },
    "2310.13575_6": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "1",
                        "Metric": "Execution Accuracy",
                        "Model": "Q \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[87.3%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "1",
                        "Support": "189"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "87.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "1",
                        "Metric": "Execution Accuracy",
                        "Model": "Q+QD \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[78.3%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "1",
                        "Support": "189"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "2",
                        "Metric": "Execution Accuracy",
                        "Model": "Q \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[86.6%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "2",
                        "Support": "277"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "86.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "2",
                        "Metric": "Execution Accuracy",
                        "Model": "Q+QD \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[83.4%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "2",
                        "Support": "277"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "3",
                        "Metric": "Execution Accuracy",
                        "Model": "Q \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[85.3%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "3",
                        "Support": "191"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "85.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "3",
                        "Metric": "Execution Accuracy",
                        "Model": "Q+QD \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[78.0%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "3",
                        "Support": "191"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "4",
                        "Metric": "Execution Accuracy",
                        "Model": "Q \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[75.0%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "4",
                        "Support": "124"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "75.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "4",
                        "Metric": "Execution Accuracy",
                        "Model": "Q+QD \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[62.9%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "4",
                        "Support": "124"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "62.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "5",
                        "Metric": "Execution Accuracy",
                        "Model": "Q \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[67.1%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "5",
                        "Support": "164"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "67.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "5",
                        "Metric": "Execution Accuracy",
                        "Model": "Q+QD \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[54.4%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "5",
                        "Support": "164"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "54.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "6",
                        "Metric": "Execution Accuracy",
                        "Model": "Q \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[48.2%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "6",
                        "Support": "27"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "48.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "6",
                        "Metric": "Execution Accuracy",
                        "Model": "Q+QD \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[25.9%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "6",
                        "Support": "27"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "25.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "7",
                        "Metric": "Execution Accuracy",
                        "Model": "Q \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[31.8%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "7",
                        "Support": "44"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "31.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "7",
                        "Metric": "Execution Accuracy",
                        "Model": "Q+QD \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[22.7%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "7",
                        "Support": "44"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "22.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "\u22658",
                        "Metric": "Execution Accuracy",
                        "Model": "Q \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[11.9%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "\u22658",
                        "Support": "18"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "11.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "\u22658",
                        "Metric": "Execution Accuracy",
                        "Model": "Q+QD \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[24.4%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "\u22658",
                        "Support": "18"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "24.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall",
                        "Metric": "Execution Accuracy",
                        "Model": "Q \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[77.4%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q \u2192 QPL",
                        "QPL Length": "Overall",
                        "Support": "1034"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "QPL Length": "Overall",
                        "Metric": "Execution Accuracy",
                        "Model": "Q+QD \u2192 QPL",
                        "Dataset": "Spider Development Set"
                    },
                    "measures": "[Execution Accuracy]",
                    "outcomes": "[69.1%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider Development Set",
                        "Model": "Q+QD \u2192 QPL",
                        "QPL Length": "Overall",
                        "Support": "1034"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "69.1"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 56,
        "total_ground_truth_claims": 18,
        "number_of_matches": 18
    },
    "1807.03100_1": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Syntactical Accuracy",
                        "Split": "Dev"
                    },
                    "measures": "[Accsyn]",
                    "outcomes": "[61.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "61.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Execution Accuracy",
                        "Split": "Dev"
                    },
                    "measures": "[Accex]",
                    "outcomes": "[72.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "72.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Syntactical Accuracy",
                        "Split": "Test"
                    },
                    "measures": "[Accsyn]",
                    "outcomes": "[62.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "62.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Execution Accuracy",
                        "Split": "Test"
                    },
                    "measures": "[Accex]",
                    "outcomes": "[71.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "71.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Syntactical Accuracy",
                        "Split": "Dev"
                    },
                    "measures": "[Accsyn]",
                    "outcomes": "[66.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Execution Accuracy",
                        "Split": "Dev"
                    },
                    "measures": "[Accex]",
                    "outcomes": "[77.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "77.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Syntactical Accuracy",
                        "Split": "Test"
                    },
                    "measures": "[Accsyn]",
                    "outcomes": "[66.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "66.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Execution Accuracy",
                        "Split": "Test"
                    },
                    "measures": "[Accex]",
                    "outcomes": "[76.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "76.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Syntactical Accuracy",
                        "Split": "Dev"
                    },
                    "measures": "[Accsyn]",
                    "outcomes": "[67.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Execution Accuracy",
                        "Split": "Dev"
                    },
                    "measures": "[Accex]",
                    "outcomes": "[78.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Syntactical Accuracy",
                        "Split": "Test"
                    },
                    "measures": "[Accsyn]",
                    "outcomes": "[67.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "67.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Execution Accuracy",
                        "Split": "Test"
                    },
                    "measures": "[Accex]",
                    "outcomes": "[78.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Pointer-SQL + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Syntactical Accuracy",
                        "Split": "Dev"
                    },
                    "measures": "[Accsyn]",
                    "outcomes": "[72.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "72.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Execution Accuracy",
                        "Split": "Dev"
                    },
                    "measures": "[Accex]",
                    "outcomes": "[79.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "79.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Syntactical Accuracy",
                        "Split": "Test"
                    },
                    "measures": "[Accsyn]",
                    "outcomes": "[71.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "71.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Execution Accuracy",
                        "Split": "Test"
                    },
                    "measures": "[Accex]",
                    "outcomes": "[78.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "78.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Syntactical Accuracy",
                        "Split": "Dev"
                    },
                    "measures": "[Accsyn]",
                    "outcomes": "[75.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Execution Accuracy",
                        "Split": "Dev"
                    },
                    "measures": "[Accex]",
                    "outcomes": "[83.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Syntactical Accuracy",
                        "Split": "Test"
                    },
                    "measures": "[Accsyn]",
                    "outcomes": "[74.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "74.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Execution Accuracy",
                        "Split": "Test"
                    },
                    "measures": "[Accex]",
                    "outcomes": "[83.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (3)",
                        "Strategy": "Execution-guided",
                        "Beam size": "3",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Syntactical Accuracy",
                        "Split": "Dev"
                    },
                    "measures": "[Accsyn]",
                    "outcomes": "[76.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "76.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Execution Accuracy",
                        "Split": "Dev"
                    },
                    "measures": "[Accex]",
                    "outcomes": "[84.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Dev",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "84.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Syntactical Accuracy",
                        "Split": "Test"
                    },
                    "measures": "[Accsyn]",
                    "outcomes": "[75.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Syntactical Accuracy"
                    ],
                    "outcomes": [
                        "75.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Dataset": "WikiSQL",
                        "Accuracy Type": "Execution Accuracy",
                        "Split": "Test"
                    },
                    "measures": "[Accex]",
                    "outcomes": "[83.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "Coarse2Fine + EG (5)",
                        "Strategy": "Execution-guided",
                        "Beam size": "5",
                        "Dataset": "WikiSQL",
                        "Split": "Test",
                        "Unit": "%"
                    },
                    "measures": [
                        "Execution Accuracy"
                    ],
                    "outcomes": [
                        "83.8"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 24,
        "total_ground_truth_claims": 24,
        "number_of_matches": 24
    },
    "2108.02866_9": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-1 Recall"
                    },
                    "measures": "[Top-1 Recall]",
                    "outcomes": "[13.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "13.10"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-5 Recall"
                    },
                    "measures": "[Top-5 Recall]",
                    "outcomes": "[20.08]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-5 Recall"
                    },
                    "measures": "[Top-5 Recall]",
                    "outcomes": "[20.08]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-5 Recall"
                    },
                    "measures": "[Top-5 Recall]",
                    "outcomes": "[20.08]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-5 Recall"
                    },
                    "measures": "[Top-5 Recall]",
                    "outcomes": "[20.08]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-5 Recall"
                    },
                    "measures": "[Top-5 Recall]",
                    "outcomes": "[20.08]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "20.08"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[22.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[22.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[22.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[22.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[22.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[22.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[22.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[22.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[22.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "22.54"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[25.24]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[25.24]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[25.24]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[25.24]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[25.24]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[25.24]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[25.24]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[25.24]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[25.24]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[25.24]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[25.24]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[25.24]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[25.24]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.24"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[29.66]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "29.66"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[33.20]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "33.20"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[13.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "13.15"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "SQuAD",
                        "Candidate Type": "textual",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[16.56]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "16.56"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-1 Recall"
                    },
                    "measures": "[Top-1 Recall]",
                    "outcomes": "[51.70]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-1 Recall"
                    },
                    "measures": "[Top-1 Recall]",
                    "outcomes": "[51.70]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "51.70"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-5 Recall"
                    },
                    "measures": "[Top-5 Recall]",
                    "outcomes": "[66.27]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-5 Recall"
                    },
                    "measures": "[Top-5 Recall]",
                    "outcomes": "[66.27]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-5 Recall"
                    },
                    "measures": "[Top-5 Recall]",
                    "outcomes": "[66.27]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-5 Recall"
                    },
                    "measures": "[Top-5 Recall]",
                    "outcomes": "[66.27]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-5 Recall"
                    },
                    "measures": "[Top-5 Recall]",
                    "outcomes": "[66.27]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "66.27"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[70.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[70.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[70.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[70.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[70.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[70.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[70.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[70.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "70.93"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[75.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[75.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[75.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[75.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[75.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[75.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[75.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[75.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[75.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[75.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[75.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "75.53"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[80.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[80.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[80.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[80.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[80.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[80.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[80.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[80.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[80.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[80.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[80.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[80.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[80.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[80.54]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.54"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[84.14]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.14"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[47.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.63"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "WikiSQL",
                        "Candidate Type": "tabular",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.49]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "AES tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.49"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-1 Recall"
                    },
                    "measures": "[Top-1 Recall]",
                    "outcomes": "[50.28]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-1 Recall"
                    },
                    "measures": "[Top-1 Recall]",
                    "outcomes": "[50.28]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-1 Recall"
                    },
                    "measures": "[Top-1 Recall]",
                    "outcomes": "[50.28]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.28"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-5 Recall"
                    },
                    "measures": "[Top-5 Recall]",
                    "outcomes": "[68.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-5 Recall"
                    },
                    "measures": "[Top-5 Recall]",
                    "outcomes": "[68.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-5 Recall"
                    },
                    "measures": "[Top-5 Recall]",
                    "outcomes": "[68.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-5 Recall"
                    },
                    "measures": "[Top-5 Recall]",
                    "outcomes": "[68.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[74.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[74.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[74.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[74.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[74.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-10 Recall"
                    },
                    "measures": "[Top-10 Recall]",
                    "outcomes": "[74.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.09"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[80.89]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[80.89]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[80.89]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[80.89]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[80.89]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[80.89]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[80.89]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[80.89]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-25 Recall"
                    },
                    "measures": "[Top-25 Recall]",
                    "outcomes": "[80.89]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.89"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[84.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[84.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[84.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[84.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[84.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[84.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[84.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[84.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[84.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[84.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-50 Recall"
                    },
                    "measures": "[Top-50 Recall]",
                    "outcomes": "[84.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.63"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[87.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[87.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[87.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[87.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[87.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[87.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[87.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[87.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[87.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[87.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[87.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[87.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "Top-100 Recall"
                    },
                    "measures": "[Top-100 Recall]",
                    "outcomes": "[87.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.13"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MAP"
                    },
                    "measures": "[MAP]",
                    "outcomes": "[44.93]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "44.93"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "18.69"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-1",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "50.24"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "25.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-5",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "68.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "28.84"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-10",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "74.10"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "32.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-25",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "80.91"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "35.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-50",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "84.78"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "38.14"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Index": "Top-100",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "Recall"
                    ],
                    "outcomes": [
                        "87.18"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "18.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MAP"
                    ],
                    "outcomes": [
                        "47.92"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked textual",
                        "Task": "Question Answering",
                        "Evidence Type": "Textual"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "22.03"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked tabular",
                        "Task": "Question Answering",
                        "Evidence Type": "Tabular"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.34"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Question Type": "NQ",
                        "Candidate Type": "hybrid",
                        "Metric": "MRR"
                    },
                    "measures": "[MRR]",
                    "outcomes": "[58.38]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "OpenWikiSQL",
                        "Model": "Reranked hybrid",
                        "Task": "Question Answering",
                        "Evidence Type": "Hybrid"
                    },
                    "measures": [
                        "MRR"
                    ],
                    "outcomes": [
                        "58.38"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 24,
        "total_ground_truth_claims": 40,
        "number_of_matches": 24
    },
    "2112.02212_3": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Augmentation Method": "DT-Fixup",
                        "Dataset": "Spider",
                        "Hardness Level": "Easy",
                        "Train Samples": "169416941694",
                        "Test Samples": "248248248"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[91.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "91.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Augmentation Method": "DT-Fixup",
                        "Dataset": "Spider",
                        "Hardness Level": "Medium",
                        "Train Samples": "277727772777",
                        "Test Samples": "446446446"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[80.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "80.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Augmentation Method": "DT-Fixup",
                        "Dataset": "Spider",
                        "Hardness Level": "Hard",
                        "Train Samples": "146114611461",
                        "Test Samples": "174174174"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[60.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "60.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Augmentation Method": "DT-Fixup",
                        "Dataset": "Spider",
                        "Hardness Level": "Extra",
                        "Train Samples": "106810681068",
                        "Test Samples": "166166166"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[48.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Augmentation Method": "DT-Fixup",
                        "Dataset": "Spider",
                        "Hardness Level": "All",
                        "Train Samples": "700070007000",
                        "Test Samples": "103410341034"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[75.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "DT-Fixup",
                        "Hardness level": "All",
                        "Train size": "7000",
                        "Test size": "1034"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "75.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Augmentation Method": "Ours",
                        "Dataset": "Spider",
                        "Hardness Level": "Easy",
                        "Train Samples": "169416941694",
                        "Test Samples": "248248248"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[92.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Easy",
                        "Train size": "1694",
                        "Test size": "248"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "92.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Augmentation Method": "Ours",
                        "Dataset": "Spider",
                        "Hardness Level": "Medium",
                        "Train Samples": "277727772777",
                        "Test Samples": "446446446"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[82.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Medium",
                        "Train size": "2777",
                        "Test size": "446"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "82.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Augmentation Method": "Ours",
                        "Dataset": "Spider",
                        "Hardness Level": "Hard",
                        "Train Samples": "146114611461",
                        "Test Samples": "174174174"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[65.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Hard",
                        "Train size": "1461",
                        "Test size": "174"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "65.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Augmentation Method": "Ours",
                        "Dataset": "Spider",
                        "Hardness Level": "Extra",
                        "Train Samples": "106810681068",
                        "Test Samples": "166166166"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[52.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "Extra",
                        "Train size": "1068",
                        "Test size": "166"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "52.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Augmentation Method": "Ours",
                        "Dataset": "Spider",
                        "Hardness Level": "All",
                        "Train Samples": "700070007000",
                        "Test Samples": "103410341034"
                    },
                    "measures": "[Accuracy]",
                    "outcomes": "[77.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "Spider",
                        "Model": "+Ours",
                        "Hardness level": "All",
                        "Train size": "7000",
                        "Test size": "1034"
                    },
                    "measures": [
                        "Accuracy"
                    ],
                    "outcomes": [
                        "77.2"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 10,
        "total_ground_truth_claims": 10,
        "number_of_matches": 10
    },
    "2310.18662_7": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "Spider",
                        "Comparison": "Inference time",
                        "Unit": "seconds per 1000 samples"
                    },
                    "measures": "[Inference Time]",
                    "outcomes": "[206.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "Spider",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "206.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "SParC",
                        "Comparison": "Inference time",
                        "Unit": "seconds per 1000 samples"
                    },
                    "measures": "[Inference Time]",
                    "outcomes": "[191.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "SParC",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "191.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "Spider",
                        "Comparison": "Inference time",
                        "Unit": "seconds per 1000 samples"
                    },
                    "measures": "[Inference Time]",
                    "outcomes": "[237.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "Spider",
                        "Comparison": "Inference time",
                        "Unit": "seconds per 1000 samples"
                    },
                    "measures": "[Inference Time]",
                    "outcomes": "[237.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "Spider",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "237.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "SParC",
                        "Comparison": "Inference time",
                        "Unit": "seconds per 1000 samples"
                    },
                    "measures": "[Inference Time]",
                    "outcomes": "[200.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "SParC",
                        "Comparison": "Inference time",
                        "Unit": "seconds per 1000 samples"
                    },
                    "measures": "[Inference Time]",
                    "outcomes": "[200.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "SParC",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "200.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "CoSQL",
                        "Comparison": "Inference time",
                        "Unit": "seconds per 1000 samples"
                    },
                    "measures": "[Inference Time]",
                    "outcomes": "[199.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "CoSQL",
                        "Comparison": "Inference time",
                        "Unit": "seconds per 1000 samples"
                    },
                    "measures": "[Inference Time]",
                    "outcomes": "[199.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ASTormer",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "199.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Metric": "Inference Time",
                        "Value": "seconds per 1000 samples"
                    },
                    "measures": "[seconds per 1000 samples]",
                    "outcomes": "[206.6, 191.5, 237.0, 200.7, 199.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "LSTM-based AST decoder",
                        "Comparison": "Inference time"
                    },
                    "measures": "[Inference Time]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Time": "seconds"
                    },
                    "measures": "[seconds]",
                    "outcomes": "[206.6, 191.5, 237.0, 200.7, 199.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Configuration": "same"
                    },
                    "measures": "[same]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Timestep": "each"
                    },
                    "measures": "[each]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Relation set": "ZjsubscriptZ_{j}"
                    },
                    "measures": "[ZjsubscriptZ_{j}]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Efficiency": "little overheads"
                    },
                    "measures": "[little overheads]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "AST decoder": "LSTM-based"
                    },
                    "measures": "[LSTM-based]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "ASTormer": ""
                    },
                    "measures": "[ASTormer]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "ZjsubscriptZ_{j}": ""
                    },
                    "measures": "[ZjsubscriptZ_{j}]",
                    "outcomes": ""
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "LSTM",
                        "Dataset": "CoSQL",
                        "Metric": "Inference time",
                        "Unit": "seconds/per 1000 samples"
                    },
                    "measures": [
                        "Time"
                    ],
                    "outcomes": [
                        "201.0"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 15,
        "total_ground_truth_claims": 6,
        "number_of_matches": 5
    },
    "2103.02227_5": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Pattern": "Seen patterns",
                        "Metric": "EM accuracy",
                        "Value": "63.5"
                    },
                    "measures": "[Metric, EM accuracy]",
                    "outcomes": "[Value, 63.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Pattern type": "Seen patterns"
                    },
                    "measures": [
                        "EM accuracy"
                    ],
                    "outcomes": [
                        "63.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Pattern": "Unseen patterns",
                        "Metric": "EM accuracy",
                        "Value": "48.8"
                    },
                    "measures": "[Metric, EM accuracy]",
                    "outcomes": "[Value, 48.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet",
                        "Dataset": "Spider",
                        "Pattern type": "Unseen patterns"
                    },
                    "measures": [
                        "EM accuracy"
                    ],
                    "outcomes": [
                        "48.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "IRNet + Aug",
                        "Pattern": "Seen patterns",
                        "Metric": "EM accuracy",
                        "Value": "64.7"
                    },
                    "measures": "[Metric, EM accuracy]",
                    "outcomes": "[Value, 64.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet + Aug",
                        "Dataset": "Spider",
                        "Pattern type": "Seen patterns",
                        "Improvement": "1.2"
                    },
                    "measures": [
                        "EM accuracy"
                    ],
                    "outcomes": [
                        "64.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "IRNet + Aug",
                        "Pattern": "Unseen patterns",
                        "Metric": "EM accuracy",
                        "Value": "53.7"
                    },
                    "measures": "[Metric, EM accuracy]",
                    "outcomes": "[Value, 53.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "IRNet + Aug",
                        "Dataset": "Spider",
                        "Pattern type": "Unseen patterns",
                        "Improvement": "4.9"
                    },
                    "measures": [
                        "EM accuracy"
                    ],
                    "outcomes": [
                        "53.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pattern": "Seen patterns",
                        "Metric": "EM accuracy",
                        "Value": "66.6"
                    },
                    "measures": "[Metric, EM accuracy]",
                    "outcomes": "[Value, 66.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Dataset": "Spider",
                        "Pattern type": "Seen patterns"
                    },
                    "measures": [
                        "EM accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Pattern": "Unseen patterns",
                        "Metric": "EM accuracy",
                        "Value": "52.3"
                    },
                    "measures": "[Metric, EM accuracy]",
                    "outcomes": "[Value, 52.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL",
                        "Dataset": "Spider",
                        "Pattern type": "Unseen patterns"
                    },
                    "measures": [
                        "EM accuracy"
                    ],
                    "outcomes": [
                        "52.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL + Aug",
                        "Pattern": "Seen patterns",
                        "Metric": "EM accuracy",
                        "Value": "73.0"
                    },
                    "measures": "[Metric, EM accuracy]",
                    "outcomes": "[Value, 73.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL + Aug",
                        "Dataset": "Spider",
                        "Pattern type": "Seen patterns",
                        "Improvement": "6.4"
                    },
                    "measures": [
                        "EM accuracy"
                    ],
                    "outcomes": [
                        "73.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "RATSQL + Aug",
                        "Pattern": "Unseen patterns",
                        "Metric": "EM accuracy",
                        "Value": "55.4"
                    },
                    "measures": "[Metric, EM accuracy]",
                    "outcomes": "[Value, 55.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "RATSQL + Aug",
                        "Dataset": "Spider",
                        "Pattern type": "Unseen patterns",
                        "Improvement": "3.1"
                    },
                    "measures": [
                        "EM accuracy"
                    ],
                    "outcomes": [
                        "55.4"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 12,
        "total_ground_truth_claims": 8,
        "number_of_matches": 8
    },
    "2305.14215_1": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[86.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "86.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[73.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[73.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[73.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[73.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[73.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[73.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[73.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "51.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[73.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[73.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "62.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[73.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[73.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "73.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "51.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "62.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "53.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "51.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "62.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[88.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "88.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "51.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "62.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "39.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "66.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[68.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "68.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "51.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "62.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "39.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "66.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.44"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "55.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "63.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.73"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "80.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "51.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "62.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "39.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "66.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.44"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "55.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "63.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.73"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "80.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.95"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "73.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.72"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[89.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "89.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "51.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "62.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "39.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "66.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.44"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "55.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "63.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.73"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "80.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.95"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "73.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.72"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "71.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "53.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "38.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "67.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[70.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "70.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "51.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "62.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "39.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "66.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.44"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "55.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "63.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.73"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "80.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.95"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "73.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.72"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "71.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "53.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "38.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "67.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.12"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "69.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.16"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "58.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.04"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "89.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "46.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "69.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "72.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.22"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "59.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[68.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.06"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "51.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "62.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "39.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "66.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.44"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "55.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "63.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.73"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "80.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.95"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "73.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.72"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "71.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "53.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "38.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "67.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.12"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "69.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.16"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "58.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.04"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "89.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "46.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "69.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "72.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.22"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "59.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.06"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "51.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "62.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "39.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "66.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.44"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "55.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "63.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.73"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "80.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.95"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "73.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.72"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "71.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "53.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "38.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "67.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.12"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "69.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.16"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "58.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.04"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "89.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "46.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "69.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "72.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.22"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "59.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Dev",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[78.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.06"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[51.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[51.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[51.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[51.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[51.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[51.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[51.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "51.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[62.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[62.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[62.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[62.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[62.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[62.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[62.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Standard Prompting",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[62.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "62.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[50.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Chain-of-Thought",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[53.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "53.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "39.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "66.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.44"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "55.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "39.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "66.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.44"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "Least-to-Most",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "63.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "39.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "66.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.44"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.73"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "80.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.95"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "73.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.72"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "71.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "53.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "38.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "67.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.12"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "69.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.16"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "58.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.04"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "89.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "46.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "69.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "72.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.22"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "59.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[55.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.06"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "39.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "66.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.44"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.73"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "80.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.95"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "73.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.72"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "71.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "53.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "38.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "67.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.12"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "69.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.16"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "58.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.04"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "89.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "46.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "69.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "72.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.22"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "59.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "QDecomp",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[65.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.06"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "39.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "66.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.44"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.73"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "80.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.95"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "73.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.72"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "71.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "53.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "38.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "67.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.12"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "69.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.16"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "58.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.04"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "89.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "46.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "69.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "72.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.22"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "59.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[56.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.06"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "39.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "66.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.44"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.73"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "80.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.95"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "73.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.72"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "71.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "53.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "38.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "67.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.12"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "69.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.16"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "58.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.04"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "89.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "46.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "69.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "72.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.22"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "59.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "Random",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Standard Execution Accuracy]",
                    "outcomes": "[63.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.06"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "65.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "50.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "36.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "63.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.08"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Standard",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "4.01"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "44.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "23.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "5.83"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "4.94"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Chain-of-Thought",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "9.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "39.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "66.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.44"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.51"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.73"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "80.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "64.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "52.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.95"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "73.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "Least-to-Most (G3)",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.72"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "71.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "53.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "38.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "67.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.89"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "7.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.12"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "69.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "3.61"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "88.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "56.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "45.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "68.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.16"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "58.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "2.04"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "70.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.54"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Easy"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "89.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Medium"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "74.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "57.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Extra Hard"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "46.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "69.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "72.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Dev",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "1.22"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Test-suite accuracy"
                    ],
                    "outcomes": [
                        "59.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Test-suite accuracy"
                    ],
                    "outcomes": [
                        "1.66"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Execution accuracy"
                    ],
                    "outcomes": [
                        "71.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Method": "+ InterCOL (G3)",
                        "Dataset": "Spider Realistic",
                        "Difficulty": "Easy",
                        "Prompting Type": "+ InterCOL",
                        "Evaluation Metric": "Standard Execution Accuracy",
                        "Experiment Type": "8-shot",
                        "In-Context Example Selection": "G3",
                        "API Doc Format": "Included"
                    },
                    "measures": "[Test-Suite Accuracy]",
                    "outcomes": "[-]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Method": "QDecomp + InterCoL + InterCoT",
                        "Dataset": "Spider Realistic",
                        "Model": "Codex",
                        "Number of shots": "8",
                        "Difficulty": "Overall"
                    },
                    "measures": [
                        "Standard deviation of Execution accuracy"
                    ],
                    "outcomes": [
                        "2.06"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 23,
        "total_ground_truth_claims": 80,
        "number_of_matches": 15
    },
    "2311.01173_3": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.29, 0.33]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.29, 0.33]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.33"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.35, 0.41]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.35, 0.41]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.35"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.45, 0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.45, 0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.45, 0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.45, 0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.57]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.57]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.57]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.57]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.57]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.57"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.48, 0.59]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.59"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.51, 0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.51, 0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.51, 0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.51, 0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.51, 0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.51, 0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.51, 0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.62"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.03, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.03, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.03, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.03, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.03, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.03, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.03, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.03, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.03, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.60"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.03, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.03, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.03, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.03, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "100",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.62"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.03, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.07"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.05, 0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.05, 0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.05, 0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.05, 0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.05, 0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.05, 0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.05, 0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.05, 0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.05, 0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.60"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.05, 0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.05, 0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.05, 0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.05, 0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "100",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.62"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.05, 0.10]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.10"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.60"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "100",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.62"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.13]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.13"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.60"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "100",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.62"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.15"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.60"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "100",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.62"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.16]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.16"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.10, 0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.10, 0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.10, 0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.10, 0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.10, 0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.10, 0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.10, 0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.10, 0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.10, 0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.60"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.10, 0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.10, 0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.10, 0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.10, 0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "100",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.62"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.10, 0.18]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.18"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.39"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.60"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.63"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "100",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.62"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.07"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.11"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.15"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.19"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.21"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.35, 0.39]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.35, 0.39]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.35, 0.39]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.35, 0.39]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.35, 0.39]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.35, 0.39]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.35, 0.39]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.39"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.46, 0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.46, 0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.46, 0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.46, 0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.46, 0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.46, 0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.46, 0.53]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.53"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.60]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.60"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.53, 0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.53, 0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.53, 0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.53, 0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.53, 0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.53, 0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.53, 0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.54, 0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.54, 0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.54, 0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.54, 0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.54, 0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.54, 0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.54, 0.64]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.64"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.63]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.63"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.52, 0.62]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "CRUSH",
                        "Budget": "100",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.62"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.04, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.04, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.04, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.04, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.04, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.04, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "3",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.04, 0.07]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.07"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "5",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.07, 0.11]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.11"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "10",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.09, 0.15]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.15"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.11, 0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.11, 0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.11, 0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.11, 0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.11, 0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.11, 0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "20",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.11, 0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.19"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.11, 0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.11, 0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.11, 0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.11, 0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.11, 0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.11, 0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "30",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.11, 0.19]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.19"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.12, 0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.12, 0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.12, 0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.12, 0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.12, 0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.12, 0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "50",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[0.12, 0.21]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.21"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "3",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.29"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "5",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Execution Match (EX) accuracy"
                    ],
                    "outcomes": [
                        "0.41"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "10",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.45"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "20",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "30",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Dataset": "BirdUnion",
                        "Method": "CRUSH (ours)",
                        "Budget": "100",
                        "Evaluation Metric": "Execution Match (EX)"
                    },
                    "measures": "[Exact Match (EM), Execution Match (EX)]",
                    "outcomes": "[-, -]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "SpiderUnion",
                        "Method": "Single DPR(OpenAI)",
                        "Budget": "50",
                        "Model": "RESDSQL"
                    },
                    "measures": [
                        "Exact Match (EM) accuracy"
                    ],
                    "outcomes": [
                        "0.48"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 28,
        "total_ground_truth_claims": 32,
        "number_of_matches": 26
    },
    "2310.13659_4": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Column Ambiguity",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[64.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "Single stage",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "64.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[60.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "Two stages",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[60.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[60.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[60.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "Single stage",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "60.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[86.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "Two stages",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[86.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[86.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[86.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "Two stages",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "66.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[86.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[86.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "67.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[86.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "Single stage",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "86.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[58.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "Two stages",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[58.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[58.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[58.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "Two stages",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "66.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[58.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[58.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "67.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[58.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "Two stages",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "88.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[58.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "87.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[58.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Single Stage",
                        "Design Decision": "Use of a single stage",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[58.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "P",
                        "Model": "Single stage",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "58.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Column Ambiguity",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[65.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "Two stages",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[66.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[66.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[66.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "Two stages",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "66.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[88.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[88.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[88.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[88.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "67.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[88.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "Two stages",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "88.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[62.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.9"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[62.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[62.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[62.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "67.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[62.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "87.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[62.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Two Stages",
                        "Design Decision": "Decoupling template and schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[62.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "P",
                        "Model": "Two stages",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "62.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Column Ambiguity",
                        "Model": "+Template Diversity",
                        "Design Decision": "Forcing counterfactual diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[65.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "+Template Diversity",
                        "Design Decision": "Forcing counterfactual diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[65.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "+Template Diversity",
                        "Design Decision": "Forcing counterfactual diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[65.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "65.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "+Template Diversity",
                        "Design Decision": "Forcing counterfactual diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[87.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "+Template Diversity",
                        "Design Decision": "Forcing counterfactual diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[87.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "67.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "+Template Diversity",
                        "Design Decision": "Forcing counterfactual diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[87.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "87.1"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "+Template Diversity",
                        "Design Decision": "Forcing counterfactual diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[63.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "+Template Diversity",
                        "Design Decision": "Forcing counterfactual diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[63.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "67.3"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "+Template Diversity",
                        "Design Decision": "Forcing counterfactual diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[63.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "+Template Diversity",
                        "Design Decision": "Forcing counterfactual diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[63.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "P",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "63.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Column Ambiguity",
                        "Model": "+Schema Diversity",
                        "Design Decision": "Encouraging schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[66.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "+Schema Diversity",
                        "Design Decision": "Encouraging schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[67.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "67.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "+Schema Diversity",
                        "Design Decision": "Encouraging schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[87.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "87.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "+Schema Diversity",
                        "Design Decision": "Encouraging schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Either In Top K (%)]",
                    "outcomes": "[64.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "P",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Either In Top K"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "64.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Column Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[23.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "Single stage",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "23.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[25.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "16.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[25.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "16.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[25.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "28.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[25.3]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "Single stage",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "25.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[54.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "16.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[54.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "16.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[54.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "28.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[54.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "28.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[54.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "28.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[54.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "42.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[54.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "Single stage",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "54.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[9.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "16.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[9.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "16.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[9.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "28.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[9.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "28.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[9.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "28.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[9.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "42.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[9.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "54.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[9.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "62.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[9.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "59.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[9.9]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "P",
                        "Model": "Single stage",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "9.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Column Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[28.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "16.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Column Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[28.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "16.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Column Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[28.0]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "28.0"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[42.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "16.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[42.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "16.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[42.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "28.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[42.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "28.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Table Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[42.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "42.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[59.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "16.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[59.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "16.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[59.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "28.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[59.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "28.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[59.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "54.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[59.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "62.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Join Ambiguity",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[59.4]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "59.4"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[24.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "16.0"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[24.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "C",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "16.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[24.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "28.4"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[24.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "T",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "28.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[24.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "54.5"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[24.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "J",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "62.2"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[24.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "P",
                        "Model": "Two stages",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "27.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[24.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "P",
                        "Model": "+Template Diversity",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "30.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Kind of Ambiguity": "Precomputed Aggregates",
                        "Model": "Both",
                        "Design Decision": "Template-guided schema diversity",
                        "Dataset": "AmbiQT"
                    },
                    "measures": "[Both In Top K (Coverage) (%)]",
                    "outcomes": "[24.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "AmbiQT",
                        "Ambiguity Type": "P",
                        "Model": "+Schema Diversity (Logical Beam)",
                        "Evaluation Type": "Both In Top K (Coverage)"
                    },
                    "measures": [
                        "Execution Match (EXM) accuracy"
                    ],
                    "outcomes": [
                        "24.8"
                    ]
                },
                "match": "yes"
            }
        ],
        "total_extracted_claims": 24,
        "total_ground_truth_claims": 32,
        "number_of_matches": 24
    },
    "2109.10540_4": {
        "matches": [
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGNPP{}_{\\text{P}}",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "None",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[37.8\u00b10.6]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.6"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "37.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGNPP{}_{\\text{P}}",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "None",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[56.9\u00b10.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.7"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "56.9"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGNPP{}_{\\text{P}}+BERT",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "BERT",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[44.7\u00b12.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGNPP{}_{\\text{P}}+BERT",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "BERT",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[44.7\u00b12.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "2.1"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "44.7"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGNPP{}_{\\text{P}}+BERT",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "BERT",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[63.8\u00b11.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGNPP{}_{\\text{P}}+BERT",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "BERT",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[63.8\u00b11.1]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.1"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "63.8"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[47.6\u00b12.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[47.6\u00b12.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[47.6\u00b12.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "2.5"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "47.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[66.6\u00b11.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[66.6\u00b11.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[66.6\u00b11.7]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.7"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "66.6"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "None",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[42.2\u00b11.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "None",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[42.2\u00b11.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "None",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[42.2\u00b11.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.3"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "53.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "None",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[42.2\u00b11.5]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.5"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "42.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "None",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[61.3\u00b10.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "None",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[61.3\u00b10.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "None",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[61.3\u00b10.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.3"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "53.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "None",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[61.3\u00b10.8]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "0.8"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "61.3"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "BERT",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[47.2\u00b11.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "BERT",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[47.2\u00b11.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "BERT",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[47.2\u00b11.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.3"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "53.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "BERT",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[47.2\u00b11.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "49.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Match",
                        "Enhancement": "BERT",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Match]",
                    "outcomes": "[47.2\u00b11.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.2"
                    },
                    "measures": [
                        "Ex.Match"
                    ],
                    "outcomes": [
                        "47.2"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "BERT",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[66.5\u00b11.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "BERT",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[66.5\u00b11.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "BERT",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[66.5\u00b11.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.3"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "53.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "BERT",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[66.5\u00b11.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "49.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT <sup></sup>",
                        "Dataset": "WTQ dev set",
                        "Metric": "Ex.Acc",
                        "Enhancement": "BERT",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[Ex.Acc]",
                    "outcomes": "[66.5\u00b11.2]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Dev",
                        "Standard Deviation": "1.2"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "66.5"
                    ]
                },
                "match": "yes"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[7.17.17.1%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[7.17.17.1%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[7.17.17.1%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.3"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "53.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[7.17.17.1%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "49.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[7.17.17.1%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.2"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "54.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[9.89.89.8%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[9.89.89.8%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[9.89.89.8%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.3"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "53.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[9.89.89.8%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "49.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[9.89.89.8%]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.2"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "54.1"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[competitive performance]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN_P",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.5"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "46.6"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[competitive performance]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "51.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[competitive performance]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "EtA+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.3"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "53.8"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[competitive performance]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.4"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "49.7"
                    ]
                },
                "match": "no"
            },
            {
                "Extracted_claim": {
                    "subject": {
                        "Model": "EtA+BERTLsubscriptBERTL\\text{BERT}_{\\text{L}}",
                        "Dataset": "Spider",
                        "Metric": "SLSQLPP{}_{\\text{P}}+BERT",
                        "Enhancement": "EtA",
                        "Schema Linking Supervision": "None"
                    },
                    "measures": "[SLSQLPP{}_{\\text{P}}+BERT]",
                    "outcomes": "[competitive performance]"
                },
                "Ground_truth_claim": {
                    "subject": {
                        "Model": "ALIGN+BERT",
                        "Dataset": "WTQ",
                        "Split": "Test",
                        "Standard Deviation": "0.2"
                    },
                    "measures": [
                        "Ex.Acc"
                    ],
                    "outcomes": [
                        "54.1"
                    ]
                },
                "match": "no"
            }
        ],
        "total_extracted_claims": 13,
        "total_ground_truth_claims": 15,
        "number_of_matches": 10
    }
}